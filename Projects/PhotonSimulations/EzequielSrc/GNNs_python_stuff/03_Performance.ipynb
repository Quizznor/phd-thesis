{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dcbdde3-fd95-418e-9fe3-e6146c07a971",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import json\n",
    "import csv\n",
    "import os\n",
    "import sys\n",
    "from pprint import pprint\n",
    "import time\n",
    "import gc \n",
    "\n",
    "# Third party imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch_geometric\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from torch_geometric.data import Dataset\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Local application imports\n",
    "sys.path.append(\"/pbs/home/e/erodrigu/TesisPhDEzequielRodriguez/Code\")\n",
    "from UHECRs_gnn import(SD433UMDatasetHomogeneous,\n",
    "                       GNNWithAttentionDiscriminator3Heads,\n",
    "                       GNNWithAttentionDiscriminator3HeadsDualInput,\n",
    "                       MaskNodes,\n",
    "                       MaskMdCounters,\n",
    "                       SilentPrunner,\n",
    "                       MaskRandomNodes,\n",
    ")\n",
    "\n",
    "from my_utils.my_basic_utils import (\n",
    "    create_bins,\n",
    "    filter_dataframe,\n",
    ")\n",
    "\n",
    "from my_utils.my_style import MyStyle\n",
    "\n",
    "# set PATHS\n",
    "code_PATH = os.path.abspath(os.path.join(\"..\"))\n",
    "project_PATH = os.path.abspath(os.path.join(code_PATH, \"..\"))\n",
    "data_PATH = os.path.join(project_PATH, \"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7285b102-34df-487f-92f4-eb7438ee1e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_weighted_histogram(df, target_col, score_col, weight_col, bins=100, density=False):\n",
    "    # Extract weights for each subset based on the target value\n",
    "    weights = df[weight_col]\n",
    "\n",
    "    # Normalize the weights\n",
    "    normalized_weights = weights / np.sum(weights)\n",
    "\n",
    "    # Compute weighted histogram with normalized weights\n",
    "    histogram, bin_edges = np.histogram(\n",
    "        df[score_col], \n",
    "        bins=bins, \n",
    "        weights=normalized_weights, \n",
    "        density=density\n",
    "    )\n",
    "\n",
    "    # Compute Poisson errors for each bin\n",
    "    def compute_poisson_errors(data, weights, bins):\n",
    "        bin_indices = np.digitize(data, bins) - 1  # Get bin index for each data point\n",
    "        bin_errors = np.array([np.sqrt(np.sum(weights[bin_indices == i] ** 2)) for i in range(len(bins) - 1)])\n",
    "        return bin_errors\n",
    "\n",
    "    errors = compute_poisson_errors(df[score_col], normalized_weights, bin_edges)\n",
    "\n",
    "    # Compute bin centers for plotting\n",
    "    bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2\n",
    "\n",
    "    return bin_centers, histogram, errors\n",
    "\n",
    "def weighted_median(values, weights):\n",
    "    # Convert inputs to lists if they are not already lists\n",
    "    if not isinstance(values, list):\n",
    "        values = list(values)\n",
    "    if not isinstance(weights, list):\n",
    "        weights = list(weights)\n",
    "\n",
    "    # Ensure values and weights have the same length\n",
    "    if len(values) != len(weights):\n",
    "        raise ValueError(\"Values and weights must have the same length.\")\n",
    "\n",
    "    # Normalize weights\n",
    "    total_weight = sum(weights)\n",
    "    normalized_weights = [w / total_weight for w in weights]\n",
    "\n",
    "    # Combine values and weights into tuples\n",
    "    data = [(values[i], normalized_weights[i]) for i in range(len(values))]\n",
    "\n",
    "    # Sort the data based on values\n",
    "    data.sort(key=lambda x: x[0])\n",
    "\n",
    "    # Calculate the total normalized weight (should be 1.0 after normalization)\n",
    "    total_normalized_weight = sum(w for v, w in data)\n",
    "    # Find the position of the weighted median\n",
    "    target_weight = total_normalized_weight / 2.0\n",
    "    cumulative_weight = 0.0\n",
    "    for value, weight in data:\n",
    "        cumulative_weight += weight\n",
    "        if cumulative_weight >= target_weight:\n",
    "            return value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c16d5d-1cbe-4f9a-820c-796f57e1836f",
   "metadata": {},
   "source": [
    "### Version check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b853d48-df9e-4535-a19d-c6475914bf49",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Torch version: {torch.__version__}\")\n",
    "print(f\"Torch CUDA version: {torch.version.cuda}\")\n",
    "print(f\"Cuda available: {torch.cuda.is_available()}\")\n",
    "print(f\"Torch geometric version: {torch_geometric.__version__}\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device is: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57db72b-3463-4b0b-bd17-b38717a3392d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c937bbaa-ef0b-4164-b3b1-60b6eaf5689e",
   "metadata": {},
   "source": [
    "### Dataset Index Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbbaae67-c649-431d-a9f5-348b4070f51d",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = \"/sps/pauger/users/erodriguez/PhotonDiscrimination/\"\n",
    "dir_path = \"/sps/pauger/users/erodriguez/PhotonDiscrimination/JSONfiles/\"\n",
    "index = pd.DataFrame()\n",
    "\n",
    "# indexes\n",
    "primaries = [\"Proton\", \"Photon\"]\n",
    "energy_bins = [\"16.5_17.0\", \"17.0_17.5\"]\n",
    "atms = [\"01\", \"03\", \"08\", \"09\"]\n",
    "indexes = [\n",
    "    f\"index_hadron_rec_{x}_{y}_{z}.csv\"\n",
    "    for x in primaries\n",
    "    for y in energy_bins\n",
    "    for z in atms\n",
    "]\n",
    "\n",
    "# create the index by appending\n",
    "for index_name in indexes:\n",
    "    proton_index = pd.read_csv(folder_path + index_name, on_bad_lines=\"skip\")\n",
    "    photon_rec_index = pd.read_csv(\n",
    "        folder_path + index_name.replace(\"hadron\", \"photon\"), on_bad_lines=\"skip\"\n",
    "    )\n",
    "    \n",
    "    index_cols = [\"filename\", \"atm_model\", \"shower_id\", \"use_id\"]\n",
    "    proton_index[index_cols] = proton_index[index_cols].astype('string')\n",
    "    photon_rec_index[index_cols] = photon_rec_index[index_cols].astype('string')\n",
    "    \n",
    "    index_ = pd.merge(\n",
    "        proton_index,\n",
    "        photon_rec_index,\n",
    "        on=index_cols,\n",
    "        how=\"left\",\n",
    "    )\n",
    "    index = pd.concat([index, index_], ignore_index=True)\n",
    "\n",
    "index = index.drop_duplicates()\n",
    "index = index.drop_duplicates(subset=[\"filename\"])\n",
    "# we won't train using iron\n",
    "index[\"mass_group\"] = index[\"filename\"].str.split(pat=\"_\", expand=True)[0]\n",
    "index = index[(index[\"mass_group\"] != \"Iron\") & (index[\"mass_group\"] != \"Helium\")]\n",
    "print(f\"Events before quality cuts: {len(index)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05342499-ec0b-4e89-9a80-b5555c164eb0",
   "metadata": {},
   "source": [
    "### Quality Cuts and Binning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c59273-303d-4264-a48f-e101b1c57b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = index.sample(frac=1)\n",
    "index.loc[index[\"filename\"].str.contains(\"Photon\"), \"isPhoton\"] = 1\n",
    "index.loc[index[\"filename\"].str.contains(\"Proton\"), \"isPhoton\"] = 0\n",
    "\n",
    "index[\"sin2zenith\"] = np.sin(index[\"zenithMC\"]) ** 2\n",
    "\n",
    "# photon efficiency from fit from simulations\n",
    "#index[\"est_efficiency\"] = (\n",
    "#    15.4074\n",
    "#    + 17.4996 * (np.log10(index[\"energyMC\"]) - 17)\n",
    "#    - 12.7485 * index[\"sin2zenith\"]\n",
    "#    - 20.7650 * index[\"sin2zenith\"] ** 2\n",
    "#    - 13.1239 * (np.log10(index[\"energyMC\"]) - 17) * index[\"sin2zenith\"]\n",
    "#)\n",
    "#index[\"est_efficiency\"] = expit(index[\"est_efficiency\"])\n",
    "\n",
    "feature_filters = {\n",
    "    \"zenithMC\": {\"filter_type\": \"range\", \"max_cut\": np.deg2rad(45)},\n",
    "    \"photon_energy\": {\"filter_type\": \"range\", \"min_cut\": 1},\n",
    "    #\"est_efficiency\": {\"filter_type\": \"range\", \"min_cut\": 0.9},\n",
    "    \"isT5\": {\"filter_type\": \"value\", \"value_to_keep\": 1}\n",
    "}\n",
    "index = filter_dataframe(index, feature_filters)\n",
    "\n",
    "index, e_bin_centers, e_bin_edges, e_labels = create_bins(\n",
    "    index,\n",
    "    lower_val=10**16.5,\n",
    "    upper_val=10**17.5,\n",
    "    num=6,\n",
    "    unbinned_col=\"energyMC\",\n",
    "    bin_column_name=\"e_bin\",\n",
    "    bin_width=\"equal_logarithmic\",\n",
    ")\n",
    "\n",
    "index, z_bin_centers, z_bin_edges, z_labels = create_bins(\n",
    "    index,\n",
    "    lower_val=0,\n",
    "    upper_val=np.sin(np.deg2rad(45)) ** 2,\n",
    "    num=4,\n",
    "    unbinned_col=\"sin2zenith\",\n",
    "    bin_column_name=\"z_bin\",\n",
    "    bin_width=\"equal\",\n",
    ")\n",
    "\n",
    "index = index.loc[~index[\"e_bin\"].isnull()]\n",
    "\n",
    "# corrupted or problematic ADSTs\n",
    "exclude_list = [\n",
    "\"Photon_17.0_17.5_011102_11\",\n",
    "\"Photon_17.0_17.5_080595_20\"\n",
    "]\n",
    "index = index[~index['filename'].isin(exclude_list)]\n",
    "\n",
    "print(f\"Events after quality cuts: {len(index)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "254cccb4-87f7-4281-9024-05fb739a2232",
   "metadata": {},
   "source": [
    "### Balanced Dataset Division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6b3780-b973-4152-855c-9307b2fc8ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine label and the two categorical variables for stratified sampling\n",
    "index[\"categorical_balance\"] = (\n",
    "    index[\"isPhoton\"].astype(str)\n",
    "    + \"_\"\n",
    "    + index[\"e_bin\"].astype(str)\n",
    "    + \"_\"\n",
    "    + index[\"z_bin\"].astype(str)\n",
    ")\n",
    "\n",
    "random_seed = 42\n",
    "stratified_split = StratifiedShuffleSplit(\n",
    "    n_splits=1, test_size=0.25, random_state=random_seed\n",
    ")\n",
    "\n",
    "for dev_index_, test_index_ in stratified_split.split(\n",
    "    index, index[\"categorical_balance\"]\n",
    "):\n",
    "    # Original Training set\n",
    "    dev_index = index.iloc[dev_index_]\n",
    "\n",
    "    # Testing set\n",
    "    test_index = index.iloc[test_index_]\n",
    "\n",
    "# Further split the original training set into train and validation sets\n",
    "validation_size = 0.25  # Adjust as needed\n",
    "split = StratifiedShuffleSplit(\n",
    "    n_splits=1, test_size=validation_size, random_state=random_seed\n",
    ")\n",
    "\n",
    "for train_index_, validation_index_ in split.split(\n",
    "    dev_index, dev_index[\"categorical_balance\"]\n",
    "):\n",
    "    train_index = dev_index.iloc[train_index_]\n",
    "    validation_index = dev_index.iloc[validation_index_]\n",
    "\n",
    "# Print the size of each dataset\n",
    "print(\"Train dataset size:\", train_index.shape[0])\n",
    "print(\"Validation dataset size:\", validation_index.shape[0])\n",
    "print(\"Test dataset size:\", test_index.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7786b2-9d47-4029-ba1d-8bc538340a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "index.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "084e9abb-2728-4d05-9e59-12ded1c644c1",
   "metadata": {},
   "source": [
    "### Heavy background df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a10509-9a2c-45b8-8076-383e91e29338",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_bg = pd.DataFrame()\n",
    "\n",
    "# indexes\n",
    "primaries = [\"Helium\", \"Iron\"]\n",
    "energy_bins = [\"16.5_17.0\", \"17.0_17.5\"]\n",
    "atms = [\"01\", \"03\", \"08\", \"09\"]\n",
    "indexes = [\n",
    "    f\"index_hadron_rec_{x}_{y}_{z}.csv\"\n",
    "    for x in primaries\n",
    "    for y in energy_bins\n",
    "    for z in atms\n",
    "]\n",
    "\n",
    "# create the index by appending\n",
    "for index_name in indexes:\n",
    "    proton_index = pd.read_csv(folder_path + index_name, on_bad_lines=\"skip\")\n",
    "    photon_rec_index = pd.read_csv(\n",
    "        folder_path + index_name.replace(\"hadron\", \"photon\"), on_bad_lines=\"skip\"\n",
    "    )\n",
    "    index_bg_ = pd.merge(\n",
    "        proton_index,\n",
    "        photon_rec_index,\n",
    "        on=[\"filename\", \"atm_model\", \"shower_id\", \"use_id\"],\n",
    "        how=\"left\",\n",
    "    )\n",
    "    index_bg = pd.concat([index_bg, index_bg_], ignore_index=True)\n",
    "\n",
    "index_bg = index_bg.drop_duplicates()\n",
    "index_bg = index_bg.drop_duplicates(subset=[\"filename\"])\n",
    "# we won't train using iron\n",
    "index_bg[\"mass_group\"] = index_bg[\"filename\"].str.split(pat=\"_\", expand=True)[0]\n",
    "\n",
    "feature_filters = {\n",
    "    \"zenithMC\": {\"filter_type\": \"range\", \"max_cut\": np.deg2rad(45)},\n",
    "    \"photon_energy\": {\"filter_type\": \"range\", \"min_cut\": 1},\n",
    "    #\"est_efficiency\": {\"filter_type\": \"range\", \"min_cut\": 0.9},\n",
    "    \"isT5\": {\"filter_type\": \"value\", \"value_to_keep\": 1}\n",
    "\n",
    "}\n",
    "\n",
    "print(f\"Events before quality cuts: {len(index_bg)}\")\n",
    "\n",
    "index_bg[\"isPhoton\"] = 0\n",
    "index_bg[\"sin2zenith\"] = np.sin(index_bg[\"zenithMC\"]) ** 2\n",
    "\n",
    "index_bg = filter_dataframe(index_bg, feature_filters)\n",
    "\n",
    "index_bg, e_bin_centers, e_bin_edges, e_labels = create_bins(\n",
    "    index_bg,\n",
    "    lower_val=10**16.5,\n",
    "    upper_val=10**17.5,\n",
    "    num=6,\n",
    "    unbinned_col=\"energyMC\",\n",
    "    bin_column_name=\"e_bin\",\n",
    "    bin_width=\"equal_logarithmic\",\n",
    ")\n",
    "\n",
    "index_bg, z_bin_centers, z_bin_edges, z_labels = create_bins(\n",
    "    index_bg,\n",
    "    lower_val=0,\n",
    "    upper_val=np.sin(np.deg2rad(45)) ** 2,\n",
    "    num=4,\n",
    "    unbinned_col=\"sin2zenith\",\n",
    "    bin_column_name=\"z_bin\",\n",
    "    bin_width=\"equal\",\n",
    ")\n",
    "\n",
    "index_bg = index_bg.loc[~index_bg[\"e_bin\"].isnull()]\n",
    "\n",
    "# problems in ADSTs\n",
    "index_bg = index_bg.sample(frac=1)\n",
    "index_bg = index_bg[(index_bg[\"mass_group\"]==\"Helium\") | (index_bg[\"mass_group\"]==\"Iron\")]\n",
    "print(f\"Events after quality cuts: {len(index_bg)}\")\n",
    "\n",
    "# problem in ADSTs\n",
    "exclude_list_bg = [\n",
    "\"Helium_16.5_17.0_010628_20\",\n",
    "\"Helium_16.5_17.0_080138_01\",\n",
    "\"Helium_17.0_17.5_011087_07\",\n",
    "\"Helium_16.5_17.0_010355_19\"\n",
    "]\n",
    "index_bg = index_bg[~index_bg['filename'].isin(exclude_list_bg)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba47953c-55f1-4086-a7a1-9a80b4af0194",
   "metadata": {},
   "source": [
    "### Generation of Normalization Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb598c7-7116-4e02-8222-b09ae9649e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths and other parameters\n",
    "dir_path = \"/sps/pauger/users/erodriguez/PhotonDiscrimination/JSONfiles/\"\n",
    "root_path = \"/sps/pauger/users/erodriguez/PhotonDiscrimination/root/\"\n",
    "\n",
    "# Function to construct paths based on DataFrame columns\n",
    "def construct_path(row, base_path):\n",
    "    return f\"{base_path}{row['mass_group']}/{row['filename']}.json\"\n",
    "\n",
    "# Set paths according to index\n",
    "#train_paths = train_index.apply(lambda row: construct_path(row, dir_path), axis=1).tolist()\n",
    "#val_paths = validation_index.apply(lambda row: construct_path(row, dir_path), axis=1).tolist()\n",
    "test_paths = test_index.apply(lambda row: construct_path(row, dir_path), axis=1).tolist()\n",
    "test_helium_paths = index_bg[index_bg[\"mass_group\"]==\"Helium\"].apply(lambda row: construct_path(row, dir_path), axis=1).tolist()\n",
    "test_iron_paths = index_bg.loc[index_bg[\"mass_group\"]==\"Iron\"].apply(lambda row: construct_path(row, dir_path), axis=1).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0337743-00a2-4ad9-89d5-ac9ee806ca95",
   "metadata": {},
   "outputs": [],
   "source": [
    "augmentation_and_normalization_options={\"mask_PMTs\":True,\n",
    "                                        \"AoP_and_saturation\":True,\n",
    "                                        \"log_normalize_traces\":True,\n",
    "                                        \"log_normalize_signals\":True,\n",
    "                                        \"mask_MD_mods\":True}\n",
    "\n",
    "normalization_dict = {\n",
    "                     # with silent\n",
    "                     #'deltaTimeHottest': {'mean': -13.743452072143555,\n",
    "                     #                     'method': 'standardization',\n",
    "                     #                     'std': 446.0340270996094},\n",
    "                     # without silent\n",
    "                     'deltaTimeHottest': {'mean': -47.27531568592806,\n",
    "                                          'method': 'standardization',\n",
    "                                          'std': 603.6062483849028},\n",
    "                     'effective_area': {'max': 31.380000000000003,\n",
    "                                        'method': 'min_max_scaling',\n",
    "                                        'min': 0},\n",
    "                     'pmt_number': {'max': 3, 'method': 'min_max_scaling', 'min': 1},\n",
    "                     #'rho_mu': {'min': -2.0,\n",
    "                     #           'method': 'min_max_scaling',\n",
    "                     #           'max': (64*3)/(3*10*np.cos(np.deg2rad(45)))},\n",
    "                     'rho_mu':{'mean': 0.21510971141596952,\n",
    "                               'method': 'standardization',\n",
    "                               'std': 0.9057438827119321},\n",
    "                     # with silent\n",
    "                     #'x': {'mean': -2.081512212753296,\n",
    "                     #      'method': 'standardization',\n",
    "                     #      'std': 779.2147827148438},\n",
    "                     #'y': {'mean': 0.3692401945590973,\n",
    "                     #      'method': 'standardization',\n",
    "                     #      'std': 779.95458984375},\n",
    "                     #'z': {'mean': -0.17404018342494965,\n",
    "                     #      'method': 'standardization',\n",
    "                     #      'std': 9.372444152832031},\n",
    "                     # without silent\n",
    "                     'x': {'mean': -1.01887806305201,\n",
    "                           'method': 'standardization',\n",
    "                           'std': 364.5171135403138},\n",
    "                     'y': {'mean': -0.06542848666299515,\n",
    "                           'method': 'standardization',\n",
    "                           'std': 356.8056580363697},\n",
    "                     'z': {'mean': -0.16543275617686548,\n",
    "                           'method': 'standardization',\n",
    "                           'std': 6.410170534866934}\n",
    "                    }\n",
    "\n",
    "\n",
    "pprint(normalization_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc7f41f-016b-48e4-8a89-287cef065802",
   "metadata": {},
   "source": [
    "### Datasets and Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3d4ff3-5eb4-41cf-9695-6e3f7b761bdd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Divide the dataset paths into subsets\n",
    "n_loaders = 12\n",
    "n_time_bins = 60\n",
    "\n",
    "dataset_args = {\"root\":root_path,\n",
    "                \"augmentation_options\":augmentation_and_normalization_options,\n",
    "                \"normalization_dict\":normalization_dict,\n",
    "                \"include_silent\":False,\n",
    "                \"n_time_bins\":60}\n",
    "\n",
    "from typing import Callable\n",
    "\n",
    "def get_loader_list(paths: list, dataset_class: Callable, dataset_args: dict = {}, n_loaders:int =10, batch_size: int =32, num_workers: int = 8):\n",
    "    subset_size = len(paths) // n_loaders\n",
    "    subset_paths = [paths[i * subset_size: (i + 1) * subset_size] for i in range(n_loaders)]\n",
    "    loaders = []\n",
    "    for subset_paths_ in subset_paths:\n",
    "        loader = DataLoader(dataset_class(file_paths=subset_paths_,\n",
    "                                  **dataset_args),\n",
    "                                  batch_size=batch_size, num_workers=num_workers)\n",
    "        loaders.append(loader)\n",
    "    return loaders\n",
    "\n",
    "# Create DataLoader lists\n",
    "test_loaders = get_loader_list(\n",
    "    paths=test_paths,\n",
    "    dataset_class=SD433UMDatasetHomogeneous,\n",
    "    dataset_args=dataset_args,\n",
    "    n_loaders=n_loaders,\n",
    "    batch_size=32,\n",
    "    num_workers=8,\n",
    ")\n",
    "\n",
    "test_helium_loaders = get_loader_list(\n",
    "    paths=test_helium_paths,\n",
    "    dataset_class=SD433UMDatasetHomogeneous,\n",
    "    dataset_args=dataset_args,\n",
    "    n_loaders=n_loaders,\n",
    "    batch_size=32,\n",
    "    num_workers=8,\n",
    ")\n",
    "\n",
    "test_iron_loaders = get_loader_list(\n",
    "    paths=test_iron_paths,\n",
    "    dataset_class=SD433UMDatasetHomogeneous,\n",
    "    dataset_args=dataset_args,\n",
    "    n_loaders=n_loaders,\n",
    "    batch_size=32,\n",
    "    num_workers=8,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc32ee89-c3be-479f-84e8-524db4d60d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(\"Parameters: \", sum(p.numel() for p in triheaded_model.parameters()))\n",
    "\n",
    "# Model ID and file paths\n",
    "test_run = False\n",
    "include_silent = False\n",
    "dual_input = True\n",
    "loss_strategy = \"equal\"  # Options: \"equal\", \"prioritize_sdmd\", \"alternate\"\n",
    "# Model ID and file paths\n",
    "model_id = f\"dual_input_{str(dual_input)}_silent_{str(include_silent)}_loss_strategy_{str(loss_strategy)}_no_TA_BN\"\n",
    "trained_model = torch.load(f\"{model_id}.pth\")\n",
    "trained_model.to(\"cpu\")\n",
    "trained_model.eval() # add check for TA batch norm mean and var"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dfe3def-e357-4c41-b2ca-abd67fb410f6",
   "metadata": {},
   "source": [
    "### Overal performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ed1818-897c-4527-881f-342c5a092ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty list to store dictionaries for DataFrame rows\n",
    "df_data = []\n",
    "\n",
    "# transformations\n",
    "device = \"cpu\"\n",
    "\n",
    "# transformations for individual input\n",
    "masking_transformation_sd =  MaskNodes(max_nodes2prune=2)\n",
    "masking_transformation_md =  MaskMdCounters(rho_mu_column_idx=7, effective_area_column_idx=6, silent_value=1/((64*3)/(3*10*np.cos(np.deg2rad(45)))-(-2.0)))\n",
    "\n",
    "# transformations for dual input\n",
    "silent_prunner = SilentPrunner(silent_col_index = 4, silent_value=0.0)\n",
    "random_masking = MaskRandomNodes(max_nodes2prune=2)\n",
    "\n",
    "for idx, test_loader in enumerate(test_loaders): \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, data in enumerate(tqdm(test_loader)):\n",
    "            try:\n",
    "                # Apply transformations\n",
    "                data_sd = masking_transformation_sd.collate(data)\n",
    "                if dual_input:\n",
    "                    # dual input\n",
    "                    #data_sd_no_silent = silent_prunner.collate(data_sd)\n",
    "                    #data_sdmd = random_masking.collate(data_sd_no_silent)\n",
    "                    data_sdmd = random_masking.collate(data_sd)\n",
    "                    #sd_graph = data_sd.to(device)\n",
    "                    #sdmd_graph = data_sdmd.to(device)\n",
    "                    inputs, targets = [data_sd, data_sdmd], data_sd.y\n",
    "                else:\n",
    "                    # single input\n",
    "                    data_sdmd = masking_transformation_md.collate(data_sd)\n",
    "                    #graph = data_sdmd.to(device)\n",
    "                    inputs, targets = data_sdmd, data_sdmd.y\n",
    "    \n",
    "                # Sanity checks on inputs and targets\n",
    "                assert torch.all(torch.isfinite(data_sd.x)), \"NaN detected in data_sd.x\"\n",
    "                assert torch.all(torch.isfinite(data_sdmd.x)), \"NaN detected in data_sdmd.x\"\n",
    "                assert torch.all(torch.isfinite(data_sd.x_traces)), \"NaN detected in data_sd.x_traces\"\n",
    "                assert torch.all(torch.isfinite(data_sdmd.x_traces)), \"NaN detected in data_sd.x_traces\"\n",
    "                assert torch.all(torch.isfinite(targets)), \"NaN detected in targets\"\n",
    "\n",
    "            except AssertionError as e:\n",
    "                # Print the error message and skip this batch\n",
    "                print(f\"Skipping batch {batch_idx} in subset {idx+1} due to assertion error: {e}\")\n",
    "                continue\n",
    "            \n",
    "            # saving preds and targets\n",
    "            output1, output2, output3 = trained_model(inputs)\n",
    "\n",
    "            # Iterate over each graph in the batch\n",
    "            for idx, (graph_id, target, prediction1, prediction2, prediction3) in enumerate(zip(data.id, targets, output1, output2, output3)):\n",
    "                # Create a dictionary for each graph's data\n",
    "                graph_sdmd_mask = idx == data_sdmd.batch\n",
    "                graph_data = {\n",
    "                    \"graph_id\": graph_id,\n",
    "                    \"target\": target.item(),\n",
    "                    \"prediction_sd\": prediction1.item(),\n",
    "                    \"prediction_sdmd\": prediction2.item(),\n",
    "                    \"prediction_md\": prediction3.item(),\n",
    "                    \"station_list\": data_sdmd.station_list[idx],\n",
    "                    \"PMT_list\": data_sdmd.x[graph_sdmd_mask,4].numpy(),\n",
    "                    \"MD_area\": data_sdmd.x[graph_sdmd_mask,6].numpy()\n",
    "                }\n",
    "                # Append the dictionary to the list\n",
    "                \n",
    "                df_data.append(graph_data)\n",
    "\n",
    "gc.collect()\n",
    "# Convert the list of dictionaries into a DataFrame\n",
    "df = pd.DataFrame(df_data)\n",
    "\n",
    "test_df = pd.merge(index, df, how='inner', left_on='filename', right_on='graph_id')\n",
    "test_df[\"spectrum_weight_gamma\"] = test_df[\"energyMC\"]**-2\n",
    "test_df[\"spectrum_weight_hadron\"] =test_df[\"energyMC\"]**-3\n",
    "test_df[\"no_weight\"] = 1\n",
    "test_df.to_csv(f\"{model_id}_test_set.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51087705-aff8-42db-b346-66d2910e9d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty list to store dictionaries for DataFrame rows\n",
    "df_data = []\n",
    "\n",
    "for idx, test_loader in enumerate(test_helium_loaders): \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, data in enumerate(tqdm(test_loader)):\n",
    "            try:\n",
    "                # Apply transformations\n",
    "                data_sd = masking_transformation_sd.collate(data)\n",
    "                if dual_input:\n",
    "                    # dual input\n",
    "                    #data_sd_no_silent = silent_prunner.collate(data_sd)\n",
    "                    #data_sdmd = random_masking.collate(data_sd_no_silent)\n",
    "                    data_sdmd = random_masking.collate(data_sd)\n",
    "                    #sd_graph = data_sd.to(device)\n",
    "                    #sdmd_graph = data_sdmd.to(device)\n",
    "                    inputs, targets = [data_sd, data_sdmd], data_sd.y\n",
    "                else:\n",
    "                    # single input\n",
    "                    data_sdmd = masking_transformation_md.collate(data_sd)\n",
    "                    #graph = data_sdmd.to(device)\n",
    "                    inputs, targets = data_sdmd, data_sdmd.y\n",
    "    \n",
    "                # Sanity checks on inputs and targets\n",
    "                assert torch.all(torch.isfinite(data_sd.x)), \"NaN detected in data_sd.x\"\n",
    "                assert torch.all(torch.isfinite(data_sdmd.x)), \"NaN detected in data_sdmd.x\"\n",
    "                assert torch.all(torch.isfinite(data_sd.x_traces)), \"NaN detected in data_sd.x_traces\"\n",
    "                assert torch.all(torch.isfinite(data_sdmd.x_traces)), \"NaN detected in data_sd.x_traces\"\n",
    "                assert torch.all(torch.isfinite(targets)), \"NaN detected in targets\"\n",
    "\n",
    "            except AssertionError as e:\n",
    "                # Print the error message and skip this batch\n",
    "                print(f\"Skipping batch {batch_idx} in subset {idx+1} due to assertion error: {e}\")\n",
    "                continue\n",
    "            \n",
    "            # saving preds and targets\n",
    "            output1, output2, output3 = trained_model(inputs)\n",
    "\n",
    "            # Iterate over each graph in the batch\n",
    "            for idx, (graph_id, target, prediction1, prediction2, prediction3) in enumerate(zip(data.id, targets, output1, output2, output3)):\n",
    "                # Create a dictionary for each graph's data\n",
    "                graph_sdmd_mask = idx == data_sdmd.batch\n",
    "                graph_data = {\n",
    "                    \"graph_id\": graph_id,\n",
    "                    \"target\": target.item(),\n",
    "                    \"prediction_sd\": prediction1.item(),\n",
    "                    \"prediction_sdmd\": prediction2.item(),\n",
    "                    \"prediction_md\": prediction3.item(),\n",
    "                    \"station_list\": data_sdmd.station_list[idx],\n",
    "                    \"PMT_list\": data_sdmd.x[graph_sdmd_mask,4].numpy(),\n",
    "                    \"MD_area\": data_sdmd.x[graph_sdmd_mask,6].numpy()\n",
    "                }\n",
    "                # Append the dictionary to the list\n",
    "                df_data.append(graph_data)\n",
    "\n",
    "gc.collect()\n",
    "# Convert the list of dictionaries into a DataFrame\n",
    "df = pd.DataFrame(df_data)\n",
    "\n",
    "test_helium_df = pd.merge(index_bg, df, how='inner', left_on='filename', right_on='graph_id')\n",
    "test_helium_df[\"spectrum_weight_hadron\"] = test_helium_df[\"energyMC\"]**-3\n",
    "test_helium_df[\"no_weight\"] = 1\n",
    "test_helium_df.to_csv(f\"{model_id}_test_set_helium.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9129a79-b5ff-4733-8122-30f6b899d2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty list to store dictionaries for DataFrame rows\n",
    "df_data = []\n",
    "\n",
    "for idx, test_loader in enumerate(test_iron_loaders): \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, data in enumerate(tqdm(test_loader)):\n",
    "            try:\n",
    "                # Apply transformations\n",
    "                data_sd = masking_transformation_sd.collate(data)\n",
    "                if dual_input:\n",
    "                    # dual input\n",
    "                    #data_sd_no_silent = silent_prunner.collate(data_sd)\n",
    "                    #data_sdmd = random_masking.collate(data_sd_no_silent)\n",
    "                    data_sdmd = random_masking.collate(data_sd)\n",
    "                    #sd_graph = data_sd.to(device)\n",
    "                    #sdmd_graph = data_sdmd.to(device)\n",
    "                    inputs, targets = [data_sd, data_sdmd], data_sd.y\n",
    "                else:\n",
    "                    # single input\n",
    "                    data_sdmd = masking_transformation_md.collate(data_sd)\n",
    "                    #graph = data_sdmd.to(device)\n",
    "                    inputs, targets = data_sdmd, data_sdmd.y\n",
    "    \n",
    "                # Sanity checks on inputs and targets\n",
    "                assert torch.all(torch.isfinite(data_sd.x)), \"NaN detected in data_sd.x\"\n",
    "                assert torch.all(torch.isfinite(data_sdmd.x)), \"NaN detected in data_sdmd.x\"\n",
    "                assert torch.all(torch.isfinite(data_sd.x_traces)), \"NaN detected in data_sd.x_traces\"\n",
    "                assert torch.all(torch.isfinite(data_sdmd.x_traces)), \"NaN detected in data_sd.x_traces\"\n",
    "                assert torch.all(torch.isfinite(targets)), \"NaN detected in targets\"\n",
    "\n",
    "            except AssertionError as e:\n",
    "                # Print the error message and skip this batch\n",
    "                print(f\"Skipping batch {batch_idx} in subset {idx+1} due to assertion error: {e}\")\n",
    "                continue\n",
    "            \n",
    "            # saving preds and targets\n",
    "            output1, output2, output3 = trained_model(inputs)\n",
    "\n",
    "            # Iterate over each graph in the batch\n",
    "            for idx, (graph_id, target, prediction1, prediction2, prediction3) in enumerate(zip(data.id, targets, output1, output2, output3)):\n",
    "                # Create a dictionary for each graph's data\n",
    "                graph_sdmd_mask = idx == data_sdmd.batch\n",
    "                graph_data = {\n",
    "                    \"graph_id\": graph_id,\n",
    "                    \"target\": target.item(),\n",
    "                    \"prediction_sd\": prediction1.item(),\n",
    "                    \"prediction_sdmd\": prediction2.item(),\n",
    "                    \"prediction_md\": prediction3.item(),\n",
    "                    \"station_list\": data_sdmd.station_list[idx],\n",
    "                    \"PMT_list\": data_sdmd.x[graph_sdmd_mask,4].numpy(),\n",
    "                    \"MD_area\": data_sdmd.x[graph_sdmd_mask,6].numpy()\n",
    "                }\n",
    "                # Append the dictionary to the list\n",
    "                df_data.append(graph_data)\n",
    "\n",
    "gc.collect()\n",
    "# Convert the list of dictionaries into a DataFrame\n",
    "df = pd.DataFrame(df_data)\n",
    "\n",
    "test_iron_df = pd.merge(index_bg, df, how='inner', left_on='filename', right_on='graph_id')\n",
    "test_iron_df[\"spectrum_weight_hadron\"] = test_iron_df[\"energyMC\"]**-3\n",
    "test_iron_df[\"no_weight\"] = 1\n",
    "test_iron_df.to_csv(f\"{model_id}_test_set_iron.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f3e018-0f35-42d9-a7b6-ad690375ccdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(\"last_dual_input_True_silent_False_loss_strategy_equal_new_60_test_set.csv\")\n",
    "test_helium_df = pd.read_csv(\"last_dual_input_True_silent_False_loss_strategy_equal_new_60_test_set_helium.csv\")\n",
    "test_iron_df = pd.read_csv(\"last_dual_input_True_silent_False_loss_strategy_equal_new_60_test_set_iron.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78cfed84-1247-42d6-8dd6-6b0a74e4ca08",
   "metadata": {},
   "outputs": [],
   "source": [
    "#myStyle = MyStyle('1fig-long', markers=None)\n",
    "from matplotlib.gridspec import GridSpec\n",
    "fig = plt.figure(figsize=(18,6))\n",
    "gs = GridSpec(2, 2, height_ratios=[1, 1], width_ratios=[1, 1])  # Define the layout\n",
    "\n",
    "# Extract weights for each subset based on the target value\n",
    "ph_df = test_df.loc[test_df[\"target\"] == 1, :]\n",
    "pr_df = test_df.loc[test_df[\"target\"] == 0, :]\n",
    "\n",
    "n_bins = 30\n",
    "fontsize = 17\n",
    "fontsize_label = 17\n",
    "label_list = [r\"$\\gamma$\", \"p\", \"He\", \"Fe\"]\n",
    "color_list = [\"blue\", \"#FF6666\", \"#CC3333\",'#4B0000']\n",
    "\n",
    "# Function to compute and plot histogram\n",
    "def plot_histogram(ax, df_list, color_list, label_list, score_col, xlabel, weights=False):\n",
    "    for idx, df in enumerate(df_list):\n",
    "        if idx==0:\n",
    "            if weights:\n",
    "                weight_col=\"spectrum_weight_gamma\"\n",
    "            else:\n",
    "                weight_col=\"no_weight\"\n",
    "                \n",
    "            weighted_median_ph = weighted_median(np.array(df[score_col]), df[weight_col])\n",
    "            bin_centers, norm_counts, errors = compute_weighted_histogram(df, target_col=\"target\",\n",
    "                                                                                   score_col=score_col,\n",
    "                                                                                   weight_col=\"spectrum_weight_gamma\",\n",
    "                                                                                   bins=n_bins)\n",
    "            ax.errorbar(bin_centers, norm_counts, yerr=errors, color=color_list[idx], label=label_list[idx], fmt='o', linestyle='')\n",
    "    \n",
    "        else:\n",
    "            if weights:\n",
    "                weight_col=\"spectrum_weight_hadron\"\n",
    "            else:\n",
    "                weight_col=\"no_weight\"\n",
    "\n",
    "            bin_centers, norm_counts, errors = compute_weighted_histogram(df, target_col=\"target\",\n",
    "                                                                               score_col=score_col,\n",
    "                                                                               weight_col=\"spectrum_weight_hadron\",\n",
    "                                                                               bins=n_bins)\n",
    "            if idx==2:\n",
    "                bin_centers = bin_centers[:-5]\n",
    "                norm_counts = norm_counts[:-5]\n",
    "                errors = errors[:-5]\n",
    "                \n",
    "            \n",
    "            ax.errorbar(bin_centers, norm_counts, yerr=errors, color=color_list[idx], label=label_list[idx], fmt=\"v\", linestyle='')\n",
    "        \n",
    "    \n",
    "    ax.axvline(x=weighted_median_ph, label=\"$\\gamma$ median\", color=\"black\", linestyle=\"dashed\")\n",
    "    ax.set_xlabel(xlabel, fontsize=fontsize)\n",
    "    ax.set_yscale(\"log\")\n",
    "    ax.set_ylim(10**-5, 1)\n",
    "\n",
    "# Plot SD\n",
    "ax1 = fig.add_subplot(gs[0, 1])\n",
    "plot_histogram(ax1, [ph_df, pr_df, test_helium_df, test_iron_df], score_col=\"prediction_sd\", label_list=label_list, color_list = color_list, weights=True, xlabel=\"SD score\")\n",
    "ax1.set_xlim(-10, 12)\n",
    "\n",
    "# Plot MD\n",
    "ax2 = fig.add_subplot(gs[1, 1])\n",
    "plot_histogram(ax2, [ph_df, pr_df, test_helium_df, test_iron_df], score_col=\"prediction_md\", label_list=label_list, color_list = color_list, weights=True, xlabel=\"UMD score\")\n",
    "ax2.set_xlim(-10, 12)\n",
    "\n",
    "# Plot SD+MD\n",
    "ax3 = fig.add_subplot(gs[:, 0])\n",
    "plot_histogram(ax3, [ph_df, pr_df, test_helium_df, test_iron_df], score_col=\"prediction_sdmd\", label_list=label_list, color_list = color_list, weights=True, xlabel=\"SD-UMD score\")\n",
    "#ax3.set_ylabel(r\"$\\text{spectrum-weighted } \\, 1/N \\times \\mathrm{d}N/\\mathrm{d}\\text{score}$\", fontsize=fontsize)\n",
    "ax3.set_ylabel(r\"$E^{-\\alpha}$ weighted norm. counts\", fontsize=fontsize)\n",
    "ax3.set_xlim(-10, 15)\n",
    "\n",
    "plt.legend(fontsize=fontsize_label, loc=\"lower right\")\n",
    "fig.savefig('hist_separation.png', dpi=300)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c149b3b8-4333-4c77-b486-08bbfbdbb237",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_feature = \"showerSize\"\n",
    "x_label = r\"$\\lg{ \\left( S_{300} \\right) }$\"'o'\n",
    "\n",
    "#myStyle = MyStyle('1fig-long', markers=None)\n",
    "fig = plt.figure(figsize=(18,6))\n",
    "gs = GridSpec(2, 2, height_ratios=[1, 1], width_ratios=[1, 1])  # Define the layout\n",
    "\n",
    "ph_df = test_df.loc[test_df[\"target\"]==1, :]\n",
    "pr_df = test_df.loc[test_df[\"target\"]==0, :]\n",
    "he_df = test_helium_df[test_helium_df[\"prediction_sdmd\"]<5]\n",
    "fe_df = test_iron_df\n",
    "\n",
    "# plot the histogram\n",
    "ax1 = fig.add_subplot(gs[0, 0])\n",
    "ax1.scatter(np.log10(ph_df[x_feature]), ph_df[\"prediction_sd\"], alpha=0.45, color=\"blue\", label='$\\gamma$', s=4)\n",
    "ax1.scatter(np.log10(pr_df[x_feature]), pr_df[\"prediction_sd\"], alpha=0.65, color=\"#FF6666\", label='p', s=4)\n",
    "ax1.scatter(np.log10(he_df[x_feature]), he_df[\"prediction_sd\"], alpha=0.65, color=\"#CC3333\", label='He', s=4)\n",
    "ax1.scatter(np.log10(fe_df[x_feature]), fe_df[\"prediction_sd\"], alpha=0.65, color='#4B0000', label='Fe', s=4)\n",
    "ax1.set_ylabel(\"SD score\")\n",
    "ax1.set_ylim(-9.0, 12)\n",
    "\n",
    "ax2 = fig.add_subplot(gs[1, 0])\n",
    "ax2.scatter(np.log10(ph_df[x_feature]), ph_df[\"prediction_md\"], alpha=0.45, color=\"blue\", label='$\\gamma$', s=4)\n",
    "ax2.scatter(np.log10(pr_df[x_feature]), pr_df[\"prediction_md\"], alpha=0.65, color=\"#FF6666\", label='p', s=4)\n",
    "ax2.scatter(np.log10(he_df[x_feature]), he_df[\"prediction_md\"], alpha=0.65, color=\"#CC3333\", label='He', s=4)\n",
    "ax2.scatter(np.log10(fe_df[x_feature]), fe_df[\"prediction_md\"], alpha=0.65, color='#4B0000', label='Fe', s=4)\n",
    "ax2.set_ylabel(\"UMD score\")\n",
    "ax2.set_xlabel(x_label)\n",
    "ax2.set_ylim(-9.0, 12)\n",
    "\n",
    "ax3 = fig.add_subplot(gs[:, 1])\n",
    "ax3.scatter(np.log10(ph_df[x_feature]), ph_df[\"prediction_sdmd\"], alpha=0.45, color=\"blue\", label='$\\gamma$', s=4)\n",
    "ax3.scatter(np.log10(pr_df[x_feature]), pr_df[\"prediction_sdmd\"], alpha=0.65, color=\"#FF6666\", label='p', s=4)\n",
    "ax3.scatter(np.log10(he_df[x_feature]), he_df[\"prediction_sdmd\"], alpha=0.65, color=\"#CC3333\", label='He', s=4)\n",
    "ax3.scatter(np.log10(fe_df[x_feature]), fe_df[\"prediction_sdmd\"], alpha=0.65, color='#4B0000', label='Fe', s=4)\n",
    "ax3.set_ylabel(\"SD-UMD score\")\n",
    "ax3.set_xlabel(x_label)\n",
    "ax3.set_ylim(-9.0, 12)\n",
    "ax3.legend(framealpha=1, ncol=4)\n",
    "\n",
    "#ax.set_ylim(-200, 350)\n",
    "for ax in [ax1, ax2, ax3]:\n",
    "    ax.set_xlim(0.0, 2.1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e412a7-ee86-425d-a89e-f9fedc868149",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_feature = \"photon_energy\"\n",
    "x_label = r\"lg($E_{\\gamma}$ / eV)\"\n",
    "fontsize = 18\n",
    "\n",
    "#myStyle = MyStyle('1fig-long', markers=None)\n",
    "fig = plt.figure(figsize=(18,6))\n",
    "gs = GridSpec(2, 2, height_ratios=[1, 1], width_ratios=[1, 1])  # Define the layout\n",
    "\n",
    "ph_df = test_df.loc[test_df[\"target\"]==1, :]\n",
    "pr_df = test_df.loc[test_df[\"target\"]==0, :]\n",
    "he_df = test_helium_df[test_helium_df[\"prediction_sdmd\"]<5]\n",
    "fe_df = test_iron_df\n",
    "\n",
    "# plot the histogram\n",
    "ax1 = fig.add_subplot(gs[0, 0])\n",
    "ax1.scatter(np.log10(ph_df[x_feature]), ph_df[\"prediction_sd\"], alpha=0.45, color=\"blue\", label='$\\gamma$', s=4)\n",
    "ax1.scatter(np.log10(pr_df[x_feature]), pr_df[\"prediction_sd\"], alpha=0.65, color=\"#FF6666\", label='p', s=4)\n",
    "ax1.scatter(np.log10(he_df[x_feature]), he_df[\"prediction_sd\"], alpha=0.65, color=\"#CC3333\", label='He', s=4)\n",
    "ax1.scatter(np.log10(fe_df[x_feature]), fe_df[\"prediction_sd\"], alpha=0.65, color='#4B0000', label='Fe', s=4)\n",
    "ax1.set_ylabel(\"SD score\", fontsize=18)\n",
    "ax1.set_ylim(-9.0, 12)\n",
    "\n",
    "ax2 = fig.add_subplot(gs[1, 0])\n",
    "ax2.scatter(np.log10(ph_df[x_feature]), ph_df[\"prediction_md\"], alpha=0.45, color=\"blue\", label='$\\gamma$', s=4)\n",
    "ax2.scatter(np.log10(pr_df[x_feature]), pr_df[\"prediction_md\"], alpha=0.65, color=\"#FF6666\", label='p', s=4)\n",
    "ax2.scatter(np.log10(he_df[x_feature]), he_df[\"prediction_md\"], alpha=0.65, color=\"#CC3333\", label='He', s=4)\n",
    "ax2.scatter(np.log10(fe_df[x_feature]), fe_df[\"prediction_md\"], alpha=0.65, color='#4B0000', label='Fe', s=4)\n",
    "ax2.set_ylabel(\"UMD score\", fontsize=18)\n",
    "ax2.set_xlabel(x_label, fontsize=18)\n",
    "ax2.set_ylim(-9.0, 12)\n",
    "\n",
    "ax3 = fig.add_subplot(gs[:, 1])\n",
    "ax3.scatter(np.log10(ph_df[x_feature]), ph_df[\"prediction_sdmd\"], alpha=0.45, color=\"blue\", label='$\\gamma$', s=4)\n",
    "ax3.scatter(np.log10(pr_df[x_feature]), pr_df[\"prediction_sdmd\"], alpha=0.65, color=\"#FF6666\", label='p', s=4)\n",
    "ax3.scatter(np.log10(he_df[x_feature]), he_df[\"prediction_sdmd\"], alpha=0.65, color=\"#CC3333\", label='He', s=4)\n",
    "ax3.scatter(np.log10(fe_df[x_feature]), fe_df[\"prediction_sdmd\"], alpha=0.65, color='#4B0000', label='Fe', s=4)\n",
    "ax3.set_ylabel(\"SD-UMD score\", fontsize=18)\n",
    "ax3.set_xlabel(x_label, fontsize=18)\n",
    "ax3.set_ylim(-9.0, 12)\n",
    "ax3.legend(framealpha=1, ncol=4, fontsize=17)\n",
    "\n",
    "#ax.set_ylim(-200, 350)\n",
    "for ax in [ax1, ax2, ax3]:\n",
    "    ax.set_xlim(16.5, 17.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "fig.savefig('scores_vs_energy.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dbd7d2e-9ce1-455a-b729-7b8673448066",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_feature = \"sin2zenith\"\n",
    "x_label = r\"$\\sin^2 \\theta$\"\n",
    "\n",
    "#myStyle = MyStyle('1fig-long', markers=None)\n",
    "fig = plt.figure(figsize=(18,6))\n",
    "gs = GridSpec(2, 2, height_ratios=[1, 1], width_ratios=[1, 1])  # Define the layout\n",
    "\n",
    "ph_df = test_df.loc[test_df[\"target\"]==1, :]\n",
    "pr_df = test_df.loc[test_df[\"target\"]==0, :]\n",
    "he_df = test_helium_df[test_helium_df[\"prediction_sdmd\"]<5]\n",
    "fe_df = test_iron_df\n",
    "\n",
    "# plot the histogram\n",
    "ax1 = fig.add_subplot(gs[0, 0])\n",
    "ax1.scatter(ph_df[x_feature], ph_df[\"prediction_sd\"], alpha=0.45, color=\"blue\", label='$\\gamma$', s=4)\n",
    "ax1.scatter(pr_df[x_feature], pr_df[\"prediction_sd\"], alpha=0.65, color=\"#FF6666\", label='p', s=4)\n",
    "ax1.scatter(he_df[x_feature], he_df[\"prediction_sd\"], alpha=0.65, color=\"#CC3333\", label='He', s=4)\n",
    "ax1.scatter(fe_df[x_feature], fe_df[\"prediction_sd\"], alpha=0.65, color='#4B0000', label='Fe', s=4)\n",
    "ax1.set_ylabel(\"SD score\", fontsize=fontsize)\n",
    "ax1.set_ylim(-9.0, 12)\n",
    "\n",
    "ax2 = fig.add_subplot(gs[1, 0])\n",
    "ax2.scatter(ph_df[x_feature], ph_df[\"prediction_md\"], alpha=0.45, color=\"blue\", label='$\\gamma$', s=4)\n",
    "ax2.scatter(pr_df[x_feature], pr_df[\"prediction_md\"], alpha=0.65, color=\"#FF6666\", label='p', s=4)\n",
    "ax2.scatter(he_df[x_feature], he_df[\"prediction_md\"], alpha=0.65, color=\"#CC3333\", label='He', s=4)\n",
    "ax2.scatter(fe_df[x_feature], fe_df[\"prediction_md\"], alpha=0.65, color='#4B0000', label='Fe', s=4)\n",
    "ax2.set_ylabel(\"UMD score\", fontsize=fontsize)\n",
    "ax2.set_xlabel(x_label, fontsize=fontsize)\n",
    "ax2.set_ylim(-9.0, 12)\n",
    "\n",
    "ax3 = fig.add_subplot(gs[:, 1])\n",
    "ax3.scatter(ph_df[x_feature], ph_df[\"prediction_sdmd\"], alpha=0.45, color=\"blue\", label='$\\gamma$', s=4)\n",
    "ax3.scatter(pr_df[x_feature], pr_df[\"prediction_sdmd\"], alpha=0.65, color=\"#FF6666\", label='p', s=4)\n",
    "ax3.scatter(he_df[x_feature], he_df[\"prediction_sdmd\"], alpha=0.65, color=\"#CC3333\", label='He', s=4)\n",
    "ax3.scatter(fe_df[x_feature], fe_df[\"prediction_sdmd\"], alpha=0.65, color='#4B0000', label='Fe', s=4)\n",
    "ax3.set_ylabel(\"SD-UMD score\", fontsize=fontsize)\n",
    "ax3.set_xlabel(x_label, fontsize=fontsize)\n",
    "ax3.set_ylim(-9.0, 12)\n",
    "ax3.legend(framealpha=1, ncol=4, fontsize=fontsize)\n",
    "\n",
    "#ax.set_ylim(-200, 350)\n",
    "for ax in [ax1, ax2, ax3]:\n",
    "    ax.set_xlim(0, 0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "fig.savefig('scores_vs_zenith.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dbc3e2e-c6fc-4eba-a496-6f8058feb69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_feature = \"Xmax\"\n",
    "x_label = r\"$X_{max}$\"\n",
    "\n",
    "#myStyle = MyStyle('1fig-long', markers=None)\n",
    "fig = plt.figure(figsize=(18,6))\n",
    "gs = GridSpec(2, 2, height_ratios=[1, 1], width_ratios=[1, 1])  # Define the layout\n",
    "\n",
    "ph_df = test_df.loc[test_df[\"target\"]==1, :]\n",
    "pr_df = test_df.loc[test_df[\"target\"]==0, :]\n",
    "he_df = test_helium_df[test_helium_df[\"prediction_sdmd\"]<5]\n",
    "fe_df = test_iron_df\n",
    "\n",
    "# plot the histogram\n",
    "ax1 = fig.add_subplot(gs[0, 0])\n",
    "ax1.scatter(ph_df[x_feature], ph_df[\"prediction_sd\"], alpha=0.45, color=\"blue\", label='$\\gamma$', s=4)\n",
    "ax1.scatter(pr_df[x_feature], pr_df[\"prediction_sd\"], alpha=0.65, color=\"#FF6666\", label='p', s=4)\n",
    "ax1.scatter(he_df[x_feature], he_df[\"prediction_sd\"], alpha=0.65, color=\"#CC3333\", label='He', s=4)\n",
    "ax1.scatter(fe_df[x_feature], fe_df[\"prediction_sd\"], alpha=0.65, color='#4B0000', label='Fe', s=4)\n",
    "ax1.set_ylabel(\"SD score\", fontsize=fontsize)\n",
    "ax1.set_ylim(-9.0, 12)\n",
    "\n",
    "ax2 = fig.add_subplot(gs[1, 0])\n",
    "ax2.scatter(ph_df[x_feature], ph_df[\"prediction_md\"], alpha=0.45, color=\"blue\", label='$\\gamma$', s=4)\n",
    "ax2.scatter(pr_df[x_feature], pr_df[\"prediction_md\"], alpha=0.65, color=\"#FF6666\", label='p', s=4)\n",
    "ax2.scatter(he_df[x_feature], he_df[\"prediction_md\"], alpha=0.65, color=\"#CC3333\", label='He', s=4)\n",
    "ax2.scatter(fe_df[x_feature], fe_df[\"prediction_md\"], alpha=0.65, color='#4B0000', label='Fe', s=4)\n",
    "ax2.set_ylabel(\"UMD score\", fontsize=fontsize)\n",
    "ax2.set_xlabel(x_label, fontsize=fontsize)\n",
    "ax2.set_ylim(-9.0, 12)\n",
    "\n",
    "ax3 = fig.add_subplot(gs[:, 1])\n",
    "ax3.scatter(ph_df[x_feature], ph_df[\"prediction_sdmd\"], alpha=0.45, color=\"blue\", label='$\\gamma$', s=4)\n",
    "ax3.scatter(pr_df[x_feature], pr_df[\"prediction_sdmd\"], alpha=0.65, color=\"#FF6666\", label='p', s=4)\n",
    "ax3.scatter(he_df[x_feature], he_df[\"prediction_sdmd\"], alpha=0.65, color=\"#CC3333\", label='He', s=4)\n",
    "ax3.scatter(fe_df[x_feature], fe_df[\"prediction_sdmd\"], alpha=0.65, color='#4B0000', label='Fe', s=4)\n",
    "ax3.set_ylabel(\"SD-UMD score\", fontsize=fontsize)\n",
    "ax3.set_xlabel(x_label, fontsize=fontsize)\n",
    "ax3.set_ylim(-9.0, 15)\n",
    "ax3.legend(framealpha=1, ncol=4, fontsize=fontsize)\n",
    "\n",
    "#ax.set_ylim(-200, 350)\n",
    "for ax in [ax1, ax2, ax3]:\n",
    "    ax.set_xlim(500, 1000)\n",
    "\n",
    "plt.tight_layout()\n",
    "fig.savefig('scores_vs_Xmax.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4352fa-9e91-4e1e-af32-e3787174342c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_feature = \"muonNumber\"\n",
    "x_label = r\"lg($N_{\\mu}$)\"\n",
    "\n",
    "#myStyle = MyStyle('1fig-long', markers=None)\n",
    "fig = plt.figure(figsize=(18,6))\n",
    "gs = GridSpec(2, 2, height_ratios=[1, 1], width_ratios=[1, 1])  # Define the layout\n",
    "\n",
    "ph_df = test_df.loc[test_df[\"target\"]==1, :]\n",
    "pr_df = test_df.loc[test_df[\"target\"]==0, :]\n",
    "he_df = test_helium_df[test_helium_df[\"prediction_sdmd\"]<5]\n",
    "fe_df = test_iron_df\n",
    "\n",
    "# plot the histogram\n",
    "ax1 = fig.add_subplot(gs[0, 0])\n",
    "ax1.scatter(np.log10(ph_df[x_feature]), ph_df[\"prediction_sd\"], alpha=0.45, color=\"blue\", label='$\\gamma$', s=4)\n",
    "ax1.scatter(np.log10(pr_df[x_feature]), pr_df[\"prediction_sd\"], alpha=0.65, color=\"#FF6666\", label='p', s=4)\n",
    "ax1.scatter(np.log10(he_df[x_feature]), he_df[\"prediction_sd\"], alpha=0.65, color=\"#CC3333\", label='He', s=4)\n",
    "ax1.scatter(np.log10(fe_df[x_feature]), fe_df[\"prediction_sd\"], alpha=0.65, color='#4B0000', label='Fe', s=4)\n",
    "ax1.set_ylabel(\"SD score\", fontsize=fontsize)\n",
    "ax1.set_ylim(-9.0, 12)\n",
    "\n",
    "ax2 = fig.add_subplot(gs[1, 0])\n",
    "ax2.scatter(np.log10(ph_df[x_feature]), ph_df[\"prediction_md\"], alpha=0.45, color=\"blue\", label='$\\gamma$', s=4)\n",
    "ax2.scatter(np.log10(pr_df[x_feature]), pr_df[\"prediction_md\"], alpha=0.65, color=\"#FF6666\", label='p', s=4)\n",
    "ax2.scatter(np.log10(he_df[x_feature]), he_df[\"prediction_md\"], alpha=0.65, color=\"#CC3333\", label='He', s=4)\n",
    "ax2.scatter(np.log10(fe_df[x_feature]), fe_df[\"prediction_md\"], alpha=0.65, color='#4B0000', label='Fe', s=4)\n",
    "ax2.set_ylabel(\"UMD score\", fontsize=fontsize)\n",
    "ax2.set_xlabel(x_label, fontsize=fontsize)\n",
    "ax2.set_ylim(-9.0, 12)\n",
    "\n",
    "ax3 = fig.add_subplot(gs[:, 1])\n",
    "ax3.scatter(np.log10(ph_df[x_feature]), ph_df[\"prediction_sdmd\"], alpha=0.45, color=\"blue\", label='$\\gamma$', s=4)\n",
    "ax3.scatter(np.log10(pr_df[x_feature]), pr_df[\"prediction_sdmd\"], alpha=0.65, color=\"#FF6666\", label='p', s=4)\n",
    "ax3.scatter(np.log10(he_df[x_feature]), he_df[\"prediction_sdmd\"], alpha=0.65, color=\"#CC3333\", label='He', s=4)\n",
    "ax3.scatter(np.log10(fe_df[x_feature]), fe_df[\"prediction_sdmd\"], alpha=0.65, color='#4B0000', label='Fe', s=4)\n",
    "ax3.set_ylabel(\"SD-UMD score\", fontsize=fontsize)\n",
    "ax3.set_xlabel(x_label, fontsize=fontsize)\n",
    "ax3.set_ylim(-9.0, 15)\n",
    "ax3.legend(framealpha=1, ncol=4, fontsize=fontsize)\n",
    "\n",
    "#ax.set_ylim(-200, 350)\n",
    "#for ax in [ax1, ax2, ax3]:\n",
    "#    ax.set_xlim(500, 1000)\n",
    "\n",
    "plt.tight_layout()\n",
    "fig.savefig('scores_vs_muon.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5121261-4e4d-4ed1-b61b-23cb2ca48609",
   "metadata": {},
   "outputs": [],
   "source": [
    "#myStyle = MyStyle('1fig-long', markers=None)\n",
    "fig, axs = plt.subplots(1, 2, figsize=(18,6))\n",
    "ph_df = test_df.loc[test_df[\"target\"]==1, :]\n",
    "pr_df = test_df.loc[test_df[\"target\"]==0, :]\n",
    "he_df = test_helium_df[test_helium_df[\"prediction_sdmd\"]<5]\n",
    "fe_df = test_iron_df\n",
    "\n",
    "xmin, xmax = -10, 25\n",
    "ymin, ymax = -10, 15\n",
    "\n",
    "axs[0].scatter(ph_df[\"prediction_sdmd\"], ph_df[\"prediction_sd\"], alpha=0.45, color=\"blue\", label='$\\gamma$', s=4)\n",
    "axs[0].scatter(pr_df[\"prediction_sdmd\"], pr_df[\"prediction_sd\"], alpha=0.65, color=\"r\", label='p', s=4)\n",
    "axs[0].scatter(test_helium_df[\"prediction_sdmd\"], test_helium_df[\"prediction_sd\"], alpha=0.65, color=\"gold\", label='He', s=4)\n",
    "axs[0].scatter(test_iron_df[\"prediction_sdmd\"], test_iron_df[\"prediction_sd\"], alpha=0.65, color=\"darkblue\", label='Fe', s=4)\n",
    "axs[0].set_ylabel(\"SD score\")\n",
    "axs[0].vlines(x=5, ymin=ymin, ymax=ymax, linestyle=\"dotted\", color=\"black\")\n",
    "axs[0].hlines(y=5, xmin=xmin, xmax=xmax, linestyle=\"dotted\", color=\"black\")\n",
    "\n",
    "axs[1].scatter(ph_df[\"prediction_sdmd\"], ph_df[\"prediction_md\"], alpha=0.45, color=\"blue\", label='$\\gamma$', s=4)\n",
    "axs[1].scatter(pr_df[\"prediction_sdmd\"], pr_df[\"prediction_md\"], alpha=0.65, color=\"r\", label='p', s=4)\n",
    "axs[1].scatter(test_helium_df[\"prediction_sdmd\"], test_helium_df[\"prediction_md\"], alpha=0.65, color=\"gold\", label='He', s=4)\n",
    "axs[1].scatter(test_iron_df[\"prediction_sdmd\"], test_iron_df[\"prediction_md\"], alpha=0.65, color=\"darkblue\", label='Fe', s=4)\n",
    "axs[1].set_ylabel(\"MD score\")\n",
    "\n",
    "for ax in axs:\n",
    "    ax.set_xlabel(\"SD-MD score\")\n",
    "    ax.set_xlim(xmin, xmax)\n",
    "    \n",
    "axs[0].set_ylim(ymin, ymax)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad5547d-02b0-4b6e-bc1a-14710c6f8c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "myStyle = MyStyle('1fig-long', markers=None)\n",
    "fig, axs = plt.subplots(1, 2)\n",
    "\n",
    "ph_df = merged_test_df.loc[merged_test_df[\"target\"]==1, :]\n",
    "pr_df = merged_test_df.loc[merged_test_df[\"target\"]==0, :]\n",
    "he_df = merged_test_df_bg\n",
    "\n",
    "xmin, xmax = -10, 25\n",
    "ymin, ymax = -10, 15\n",
    "\n",
    "axs[0].scatter(ph_df[\"prediction_sdmd\"], ph_df[\"M1\"], alpha=0.45, color=\"blue\", label='$\\gamma$', s=4)\n",
    "axs[0].scatter(pr_df[\"prediction_sdmd\"], pr_df[\"M1\"], alpha=0.65, color=\"r\", label='p', s=4)\n",
    "axs[0].scatter(he_df[\"prediction_sdmd\"], he_df[\"M1\"], alpha=0.65, color=\"gold\", label='p', s=4)\n",
    "axs[0].set_ylabel(\"SD score\")\n",
    "#axs[0].vlines(x=5, ymin=ymin, ymax=ymax, linestyle=\"dotted\", color=\"black\")\n",
    "#axs[0].hlines(y=5, xmin=xmin, xmax=xmax, linestyle=\"dotted\", color=\"black\")\n",
    "\n",
    "axs[1].scatter(ph_df[\"prediction_md\"], ph_df[\"M1\"], alpha=0.45, color=\"blue\", label='$\\gamma$', s=4)\n",
    "axs[1].scatter(pr_df[\"prediction_md\"], pr_df[\"M1\"], alpha=0.65, color=\"r\", label='p', s=4)\n",
    "axs[1].scatter(he_df[\"prediction_md\"], he_df[\"M1\"], alpha=0.65, color=\"gold\", label='p', s=4)\n",
    "axs[1].set_ylabel(\"MD score\")\n",
    "\n",
    "for ax in axs:\n",
    "    ax.set_xlabel(\"SD-MD score\")\n",
    "    #ax.set_xlim(xmin, xmax)\n",
    "    \n",
    "#axs[0].set_ylim(ymin, ymax)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6388c455-b691-4959-ba64-804317b07a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "myStyle = MyStyle('1fig-long', markers=None)\n",
    "fig, axs = plt.subplots(1, 2)\n",
    "\n",
    "ph_df = merged_test_df.loc[merged_test_df[\"target\"]==1, :]\n",
    "pr_df = merged_test_df.loc[merged_test_df[\"target\"]==0, :]\n",
    "he_df = merged_test_df_bg\n",
    "\n",
    "xmin, xmax = -10, 25\n",
    "ymin, ymax = -10, 15\n",
    "\n",
    "axs[0].scatter(ph_df[\"showerSize\"], -1*ph_df[\"M1\"], alpha=0.45, color=\"blue\", label='$\\gamma$', s=4)\n",
    "axs[0].scatter(pr_df[\"showerSize\"], -1*pr_df[\"M1\"], alpha=0.65, color=\"r\", label='p', s=4)\n",
    "axs[0].scatter(he_df[\"showerSize\"], -1*he_df[\"M1\"], alpha=0.65, color=\"gold\", label='p', s=4)\n",
    "axs[0].set_xlabel(r\"$\\lg{ \\left( S_{300} \\right) }$\")\n",
    "\n",
    "axs[1].scatter(ph_df[\"sin2zenith\"], -1*ph_df[\"M1\"], alpha=0.45, color=\"blue\", label='$\\gamma$', s=4)\n",
    "axs[1].scatter(pr_df[\"sin2zenith\"], -1*pr_df[\"M1\"], alpha=0.65, color=\"r\", label='p', s=4)\n",
    "axs[1].scatter(he_df[\"sin2zenith\"], -1*he_df[\"M1\"], alpha=0.65, color=\"gold\", label='p', s=4)\n",
    "axs[1].set_xlabel(r\"$\\sin^2{\\theta_{\\text{MC}}}$\")\n",
    "\n",
    "for ax in axs:\n",
    "    ax.set_ylabel(\"M1\")\n",
    "    #ax.set_xlim(xmin, xmax)\n",
    "    \n",
    "#axs[0].set_ylim(ymin, ymax)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bba9e27-b28a-4bcf-97a8-b72173933650",
   "metadata": {},
   "outputs": [],
   "source": [
    "FP = merged_test_df.loc[(merged_test_df[\"prediction_sdmd\"]>5) & (merged_test_df[\"isPhoton\"]==0), :]\n",
    "print(f\"{FP.shower_id.unique()}\")\n",
    "FP.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a84998f-3129-406c-9689-f882ec26ebd9",
   "metadata": {},
   "source": [
    "## Missing hottest Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2535cebf-8329-44fb-9864-04522fb911a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import Dataset, Data, Batch\n",
    "from torch_geometric.transforms import BaseTransform\n",
    "from torch_geometric.utils import subgraph, dropout_node\n",
    "\n",
    "class MaskHottest(BaseTransform):\n",
    "    def __init__(self):\n",
    "        super(MaskHottest, self).__init__()\n",
    "\n",
    "    def __call__(self, input_data):\n",
    "        \"\"\"\n",
    "        Prune hottest station from graph.\n",
    "\n",
    "        Returns:\n",
    "            torch_geometric.data.Data: The data object without the hottest station.\n",
    "        \"\"\"\n",
    "        \n",
    "        num_nodes = input_data.num_nodes\n",
    "        data = input_data.clone()\n",
    "        nodes2keep = torch.arange(1, num_nodes)\n",
    "        # Update the feature matrix, x_traces, total_signal, station_list, and pos to keep only selected nodes\n",
    "        data.x = data.x[nodes2keep, :]\n",
    "        data.x_traces = data.x_traces[nodes2keep, :]\n",
    "        #data.total_signal = data.total_signal[nodes2keep, :]\n",
    "        data.station_list = [data.station_list[node] for node in nodes2keep]\n",
    "        data.distance2hottest_list = [data.distance2hottest_list[node] for node in nodes2keep]\n",
    "        data.pos = data.pos[nodes2keep]\n",
    "\n",
    "        # Use subgraph method to get the new edge_index\n",
    "        new_edge_index, _ = subgraph(nodes2keep, edge_index=data.edge_index.long(), relabel_nodes=True, num_nodes=num_nodes)\n",
    "        data.edge_index = new_edge_index\n",
    "\n",
    "        # Update the number of nodes in the data object\n",
    "        data.num_nodes = data.x.size(0)\n",
    "\n",
    "        return data\n",
    "\n",
    "    def collate(self, batch):\n",
    "        \n",
    "        new_graphs = []\n",
    "        for graph_batch_id in np.unique(batch.batch):\n",
    "\n",
    "            # get features\n",
    "            graph_mask = graph_batch_id == batch.batch\n",
    "            _graph_x = batch.x[graph_mask]\n",
    "            _graph_y = batch.y[graph_batch_id]\n",
    "            _graph_x_traces = batch.x_traces[graph_mask]\n",
    "            #_total_signal =  batch.total_signal[graph_mask]\n",
    "            _station_list = batch.station_list[graph_batch_id]\n",
    "            _distance2hottest_list = batch.distance2hottest_list[graph_batch_id]\n",
    "            _pos = batch.pos[graph_mask]\n",
    "\n",
    "            # get graph edges from batch\n",
    "            nodes2keep = torch.tensor([x for x in range(batch.ptr[graph_batch_id], batch.ptr[graph_batch_id+1])])\n",
    "            _graph_x_edge_index, _ = subgraph(nodes2keep, edge_index=batch.edge_index.long(), relabel_nodes=True, num_nodes=len(batch.batch))\n",
    "            \n",
    "            # the new graph or \"subgraph\"\n",
    "            new_graph = Data(x=_graph_x, edge_index = _graph_x_edge_index, pos=_pos)\n",
    "            new_graph.x_traces = _graph_x_traces\n",
    "            #new_graph.total_signal = _total_signal\n",
    "            new_graph.station_list = _station_list\n",
    "            new_graph.distance2hottest_list = _distance2hottest_list\n",
    "            new_graph.y = _graph_y\n",
    "            \n",
    "            new_graphs.append(new_graph)\n",
    "        \n",
    "        # Now, use the Batch class to collect the transformed graphs into a batch\n",
    "        return Batch.from_data_list([self(graph) for graph in new_graphs])\n",
    "    \n",
    "masking_transformation = MaskHottest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f16bb63-adc1-4d64-938e-9d9f3ce5920b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty list to store dictionaries for DataFrame rows\n",
    "df_data = []\n",
    "\n",
    "# transformations\n",
    "masking_transformation = MaskHottest()\n",
    "\n",
    "for idx, test_loader in enumerate(test_loaders): \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, data in enumerate(tqdm(test_loader)):\n",
    "            sd_graph = masking_transformation_sd.collate(data)\n",
    "            sdmd_graph = masking_transformation.collate(sd_graph)\n",
    "            sd_graph.x = sd_graph.x[:, :sd_node_features]\n",
    "            inputs, targets = [sd_graph, sdmd_graph], sd_graph.y\n",
    "            # saving preds and targets\n",
    "            output1, output2, output3 = trained_model(inputs)\n",
    "\n",
    "            # Iterate over each graph in the batch\n",
    "            for idx, (graph_id, target, prediction1, prediction2, prediction3) in enumerate(zip(data.id, targets, output1, output2, output3)):\n",
    "                # Create a dictionary for each graph's data\n",
    "                graph_mask_sd = idx == sd_graph.batch\n",
    "                graph_mask_sdmd = idx == sdmd_graph.batch\n",
    "                graph_data = {\n",
    "                    \"graph_id\": graph_id,\n",
    "                    \"target\": target.item(),\n",
    "                    \"prediction_sd\": prediction1.item(),\n",
    "                    \"prediction_sdmd\": prediction2.item(),\n",
    "                    \"prediction_md\": prediction3.item(),\n",
    "                    \"original_station_list\": data.station_list[idx],\n",
    "                    \"sd_station_list\": sd_graph.station_list[idx],\n",
    "                    \"sdmd_station_list\": sdmd_graph.station_list[idx],\n",
    "                    \"PMT_list_sd\": sd_graph.x[graph_mask_sd,4].numpy(),\n",
    "                    \"PMT_list_sdmd\": sdmd_graph.x[graph_mask_sdmd,4].numpy(),\n",
    "                    \"MD_area_list\": sdmd_graph.x[graph_mask_sdmd,5].numpy(),\n",
    "                    \"sdmd_distance2hottest\": sdmd_graph.distance2hottest_list[idx],\n",
    "                }\n",
    "                # Append the dictionary to the list\n",
    "                df_data.append(graph_data)\n",
    "\n",
    "gc.collect()\n",
    "# Convert the list of dictionaries into a DataFrame\n",
    "df = pd.DataFrame(df_data)\n",
    "\n",
    "# Merge the two DataFrames on the \"filename\" column\n",
    "df = pd.DataFrame(df_data)\n",
    "df[\"orig_stations\"] = df[\"original_station_list\"].apply(lambda x: len(x))\n",
    "df[\"sd_stations\"] = df[\"sd_station_list\"].apply(lambda x: len(x))\n",
    "df[\"sdmd_stations\"] = df[\"sdmd_station_list\"].apply(lambda x: len(x))\n",
    "\n",
    "merged_test_df_wohottest = pd.merge(index, df, how='inner', left_on='filename', right_on='graph_id')\n",
    "merged_test_df_wohottest[\"spectrum_weight_gamma\"] = merged_test_df_wohottest[\"energyMC\"]**-2\n",
    "merged_test_df_wohottest[\"spectrum_weight_hadron\"] = merged_test_df_wohottest[\"energyMC\"]**-3\n",
    "merged_test_df_wohottest[\"no_weight\"] = 1\n",
    "merged_test_df_wohottest['n_1stcrown_stations'] = merged_test_df_wohottest['sdmd_distance2hottest'].apply(count_below_cutoff, cutoff=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6af74e-6bcf-4eab-aeb8-ec977305f970",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty list to store dictionaries for DataFrame rows\n",
    "df_data_bg = []\n",
    "\n",
    "# transformations\n",
    "masking_transformation = MaskHottest()\n",
    "\n",
    "for idx, test_loader in enumerate(test_bg_loaders): \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, data in enumerate(tqdm(test_loader)):\n",
    "            sd_graph = masking_transformation_sd.collate(data)\n",
    "            sdmd_graph = masking_transformation.collate(sd_graph)\n",
    "            sd_graph.x = sd_graph.x[:, :sd_node_features]\n",
    "            inputs, targets = [sd_graph, sdmd_graph], sd_graph.y\n",
    "            # saving preds and targets\n",
    "            output1, output2, output3 = trained_model(inputs)\n",
    "\n",
    "            # Iterate over each graph in the batch\n",
    "            for idx, (graph_id, target, prediction1, prediction2, prediction3) in enumerate(zip(data.id, targets, output1, output2, output3)):\n",
    "                # Create a dictionary for each graph's data\n",
    "                graph_mask_sd = idx == sd_graph.batch\n",
    "                graph_mask_sdmd = idx == sdmd_graph.batch\n",
    "                graph_data = {\n",
    "                    \"graph_id\": graph_id,\n",
    "                    \"target\": target.item(),\n",
    "                    \"prediction_sd\": prediction1.item(),\n",
    "                    \"prediction_sdmd\": prediction2.item(),\n",
    "                    \"prediction_md\": prediction3.item(),\n",
    "                    \"original_station_list\": data.station_list[idx],\n",
    "                    \"sd_station_list\": sd_graph.station_list[idx],\n",
    "                    \"sdmd_station_list\": sdmd_graph.station_list[idx],\n",
    "                    \"PMT_list_sd\": sd_graph.x[graph_mask_sd,4].numpy(),\n",
    "                    \"PMT_list_sdmd\": sdmd_graph.x[graph_mask_sdmd,4].numpy(),\n",
    "                    \"MD_area_list\": sdmd_graph.x[graph_mask_sdmd,5].numpy(),\n",
    "                    \"sdmd_distance2hottest\": sdmd_graph.distance2hottest_list[idx],\n",
    "                }\n",
    "                # Append the dictionary to the list\n",
    "                df_data_bg.append(graph_data)\n",
    "\n",
    "gc.collect()\n",
    "# Convert the list of dictionaries into a DataFrame\n",
    "df_bg = pd.DataFrame(df_data_bg)\n",
    "\n",
    "# Merge the two DataFrames on the \"filename\" column\n",
    "df_bg = pd.DataFrame(df_data_bg)\n",
    "df_bg[\"orig_stations\"] = df_bg[\"original_station_list\"].apply(lambda x: len(x))\n",
    "df_bg[\"sd_stations\"] = df_bg[\"sd_station_list\"].apply(lambda x: len(x))\n",
    "df_bg[\"sdmd_stations\"] = df_bg[\"sdmd_station_list\"].apply(lambda x: len(x))\n",
    "\n",
    "merged_test_df_bg_wohottest = pd.merge(index_bg, df_bg, how='inner', left_on='filename', right_on='graph_id')\n",
    "merged_test_df_bg_wohottest[\"spectrum_weight_gamma\"] = merged_test_df_bg_wohottest[\"energyMC\"]**-2\n",
    "merged_test_df_bg_wohottest[\"spectrum_weight_hadron\"] = merged_test_df_bg_wohottest[\"energyMC\"]**-3\n",
    "merged_test_df_bg_wohottest[\"no_weight\"] = 1\n",
    "merged_test_df_bg_wohottest['n_1stcrown_stations'] = merged_test_df_bg_wohottest['sdmd_distance2hottest'].apply(count_below_cutoff, cutoff=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ac7fa7-05d4-4c4c-b806-ffa4a2b58839",
   "metadata": {},
   "outputs": [],
   "source": [
    "myStyle = MyStyle('1fig-long', markers=None)\n",
    "fig, axs = plt.subplots(1, 2)\n",
    "\n",
    "\n",
    "linestyles = [\"solid\",\"dotted\"]\n",
    "labels = [\"w. hottest counter\", \"w.o. hottest counter\"]\n",
    "\n",
    "for idx, (dataset, dataset_bg) in enumerate([(merged_test_df, merged_test_df_bg), (merged_test_df_wohottest, merged_test_df_bg_wohottest)]):\n",
    "\n",
    "    # Extract weights for each subset based on the target value\n",
    "    ph_df = dataset.loc[dataset[\"target\"] == 1, :]\n",
    "    pr_df = dataset.loc[dataset[\"target\"] == 0, :]\n",
    "    he_df = dataset_bg\n",
    "\n",
    "    n_bins = 30\n",
    "    fontsize = 20\n",
    "    fontsize_label = 18\n",
    "    linestyle = linestyles[idx]\n",
    "    \n",
    "    for idy, score_col in enumerate([\"prediction_sdmd\", \"prediction_md\"]):\n",
    "\n",
    "        ph_bin_centers, ph_norm_counts, ph_errors = compute_weighted_histogram(ph_df, target_col=\"target\",\n",
    "                                                                               score_col=score_col,\n",
    "                                                                               weight_col=\"spectrum_weight_gamma\",\n",
    "                                                                               bins=n_bins)\n",
    "        pr_bin_centers, pr_norm_counts, pr_errors = compute_weighted_histogram(pr_df, target_col=\"target\",\n",
    "                                                                               score_col=score_col,\n",
    "                                                                               weight_col=\"spectrum_weight_hadron\",\n",
    "                                                                               bins=n_bins)\n",
    "        he_bin_centers, he_norm_counts, he_errors = compute_weighted_histogram(he_df, target_col=\"target\",\n",
    "                                                                               score_col=score_col,\n",
    "                                                                               weight_col=\"spectrum_weight_hadron\",\n",
    "                                                                               bins=n_bins)\n",
    "\n",
    "        weighted_median_ph = weighted_median(np.array(ph_df[score_col]), ph_df[weight_cols[0]])\n",
    "        axs[idy].step(pr_bin_centers, pr_norm_counts, where='mid', color=\"red\", label=f'p - {labels[idx]}', linestyle=linestyle)\n",
    "        axs[idy].step(ph_bin_centers, ph_norm_counts, where='mid', color=\"blue\", label=f'$\\gamma$ - {labels[idx]}', linestyle=linestyle)\n",
    "        #axs[idy].step(he_bin_centers, he_norm_counts, where='mid', color=\"gold\", label='He', linestyle=linestyle)\n",
    "        #axs[idy].axvline(x=weighted_median_ph, label=\"$\\gamma$ median\", color=\"black\", linestyle=linestyle)\n",
    "\n",
    "        \n",
    "axs[0].set_xlabel(\"SD-MD score\"); axs[1].set_xlabel(\"MD score\")\n",
    "\n",
    "for ax in axs:\n",
    "    ax.set_yscale(\"log\")\n",
    "    ax.set_ylim(10**-5, 1)\n",
    "    ax.legend(fontsize=fontsize_label, ncol=2, framealpha=1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyG",
   "language": "python",
   "name": "pyg"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
