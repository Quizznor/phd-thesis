{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4dcbdde3-fd95-418e-9fe3-e6146c07a971",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import json\n",
    "import csv\n",
    "import os\n",
    "import sys\n",
    "from pprint import pprint\n",
    "import time\n",
    "import gc \n",
    "\n",
    "# Third party imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch_geometric\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from torch_geometric.data import Dataset\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Local application imports\n",
    "sys.path.append(\"/pbs/home/e/erodrigu/TesisPhDEzequielRodriguez/Code\")\n",
    "from UHECRs_gnn import(SD433UMDatasetHomogeneous,\n",
    "                       GNNWithAttentionDiscriminator3Heads,\n",
    "                       GNNWithAttentionDiscriminator3HeadsDualInput,\n",
    "                       MaskNodes,\n",
    "                       MaskMdCounters,\n",
    "                       SilentPrunner,\n",
    "                       MaskRandomNodes,\n",
    ")\n",
    "\n",
    "from my_utils.my_basic_utils import (\n",
    "    create_bins,\n",
    "    filter_dataframe,\n",
    ")\n",
    "\n",
    "# set PATHS\n",
    "code_PATH = os.path.abspath(os.path.join(\"..\"))\n",
    "project_PATH = os.path.abspath(os.path.join(code_PATH, \"..\"))\n",
    "data_PATH = os.path.join(project_PATH, \"data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c16d5d-1cbe-4f9a-820c-796f57e1836f",
   "metadata": {},
   "source": [
    "### Version check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8b853d48-df9e-4535-a19d-c6475914bf49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: 1.11.0\n",
      "Torch CUDA version: 10.2\n",
      "Cuda available: True\n",
      "Torch geometric version: 2.4.0\n",
      "Device is: cuda\n"
     ]
    }
   ],
   "source": [
    "print(f\"Torch version: {torch.__version__}\")\n",
    "print(f\"Torch CUDA version: {torch.version.cuda}\")\n",
    "print(f\"Cuda available: {torch.cuda.is_available()}\")\n",
    "print(f\"Torch geometric version: {torch_geometric.__version__}\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device is: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a57db72b-3463-4b0b-bd17-b38717a3392d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Jan  2 00:39:21 2025       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 470.256.02   Driver Version: 470.256.02   CUDA Version: 11.4     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla K80           Off  | 00000000:04:00.0 Off |                    0 |\n",
      "| N/A   23C    P8    27W / 149W |      3MiB / 11441MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c937bbaa-ef0b-4164-b3b1-60b6eaf5689e",
   "metadata": {},
   "source": [
    "### Dataset Index Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bbbaae67-c649-431d-a9f5-348b4070f51d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Events before quality cuts: 398227\n"
     ]
    }
   ],
   "source": [
    "folder_path = \"/sps/pauger/users/erodriguez/PhotonDiscrimination/\"\n",
    "dir_path = \"/sps/pauger/users/erodriguez/PhotonDiscrimination/JSONfiles/\"\n",
    "index = pd.DataFrame()\n",
    "\n",
    "# indexes\n",
    "primaries = [\"Proton\", \"Photon\"]\n",
    "energy_bins = [\"16.5_17.0\", \"17.0_17.5\"]\n",
    "atms = [\"01\", \"03\", \"08\", \"09\"]\n",
    "indexes = [\n",
    "    f\"index_hadron_rec_{x}_{y}_{z}.csv\"\n",
    "    for x in primaries\n",
    "    for y in energy_bins\n",
    "    for z in atms\n",
    "]\n",
    "\n",
    "# create the index by appending\n",
    "for index_name in indexes:\n",
    "    proton_index = pd.read_csv(folder_path + index_name, on_bad_lines=\"skip\")\n",
    "    photon_rec_index = pd.read_csv(\n",
    "        folder_path + index_name.replace(\"hadron\", \"photon\"), on_bad_lines=\"skip\"\n",
    "    )\n",
    "    index_ = pd.merge(\n",
    "        proton_index,\n",
    "        photon_rec_index,\n",
    "        on=[\"filename\", \"atm_model\", \"shower_id\", \"use_id\"],\n",
    "        how=\"left\",\n",
    "    )\n",
    "    index = pd.concat([index, index_], ignore_index=True)\n",
    "\n",
    "index = index.drop_duplicates()\n",
    "index = index.drop_duplicates(subset=[\"filename\"])\n",
    "# we won't train using iron\n",
    "index[\"mass_group\"] = index[\"filename\"].str.split(pat=\"_\", expand=True)[0]\n",
    "index = index[index[\"mass_group\"] != \"Iron\"]\n",
    "print(f\"Events before quality cuts: {len(index)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "87593045-cbbb-4c54-bf4c-9ffd76e85622",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/sps/pauger/users/erodriguez/envs/pyg/lib/python3.8/site-packages/numpy/lib/function_base.py:4573: RuntimeWarning: invalid value encountered in subtract\n",
      "  diff_b_a = subtract(b, a)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>atm_model</th>\n",
       "      <th>shower_id</th>\n",
       "      <th>use_id</th>\n",
       "      <th>energyMC</th>\n",
       "      <th>zenithMC</th>\n",
       "      <th>showerSize</th>\n",
       "      <th>showerSizeError</th>\n",
       "      <th>isT5</th>\n",
       "      <th>is6T5</th>\n",
       "      <th>Xmax</th>\n",
       "      <th>...</th>\n",
       "      <th>nearestid</th>\n",
       "      <th>nCandidates</th>\n",
       "      <th>bLDF</th>\n",
       "      <th>isSaturated</th>\n",
       "      <th>muonNumber</th>\n",
       "      <th>electromagneticEnergy</th>\n",
       "      <th>photon_energy</th>\n",
       "      <th>s_250</th>\n",
       "      <th>equivalent_energy</th>\n",
       "      <th>M1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>398227.000000</td>\n",
       "      <td>398227.000000</td>\n",
       "      <td>398227.000000</td>\n",
       "      <td>3.982270e+05</td>\n",
       "      <td>398227.000000</td>\n",
       "      <td>398227.000000</td>\n",
       "      <td>398227.000000</td>\n",
       "      <td>398227.000000</td>\n",
       "      <td>398227.000000</td>\n",
       "      <td>398227.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>398227.0</td>\n",
       "      <td>398227.000000</td>\n",
       "      <td>398227.000000</td>\n",
       "      <td>398227.000000</td>\n",
       "      <td>3.982270e+05</td>\n",
       "      <td>3.982270e+05</td>\n",
       "      <td>3.981870e+05</td>\n",
       "      <td>398187.000000</td>\n",
       "      <td>3.979650e+05</td>\n",
       "      <td>3.979650e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>5.250465</td>\n",
       "      <td>624.671403</td>\n",
       "      <td>10.489314</td>\n",
       "      <td>1.238382e+17</td>\n",
       "      <td>0.677721</td>\n",
       "      <td>16.222010</td>\n",
       "      <td>1.188039</td>\n",
       "      <td>0.671835</td>\n",
       "      <td>0.671835</td>\n",
       "      <td>731.265860</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.788909</td>\n",
       "      <td>0.761453</td>\n",
       "      <td>0.116549</td>\n",
       "      <td>4.892502e+05</td>\n",
       "      <td>1.139049e+17</td>\n",
       "      <td>1.642059e+17</td>\n",
       "      <td>29.006530</td>\n",
       "      <td>1.202942e+17</td>\n",
       "      <td>-inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3.345009</td>\n",
       "      <td>360.883823</td>\n",
       "      <td>5.766413</td>\n",
       "      <td>7.912391e+16</td>\n",
       "      <td>0.273104</td>\n",
       "      <td>17.278987</td>\n",
       "      <td>0.866914</td>\n",
       "      <td>0.469546</td>\n",
       "      <td>0.469546</td>\n",
       "      <td>97.612257</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.378215</td>\n",
       "      <td>0.426196</td>\n",
       "      <td>0.320883</td>\n",
       "      <td>5.960837e+05</td>\n",
       "      <td>7.390093e+16</td>\n",
       "      <td>8.526831e+18</td>\n",
       "      <td>32.499657</td>\n",
       "      <td>6.294315e+17</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.162770e+16</td>\n",
       "      <td>0.005836</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>541.340000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.914030e+03</td>\n",
       "      <td>2.378890e+16</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>-inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>312.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.628890e+16</td>\n",
       "      <td>0.470053</td>\n",
       "      <td>3.482985</td>\n",
       "      <td>0.674117</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>674.485000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.835660e+04</td>\n",
       "      <td>5.133390e+16</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>5.067435</td>\n",
       "      <td>3.012160e+16</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>8.000000</td>\n",
       "      <td>625.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>9.965710e+16</td>\n",
       "      <td>0.698197</td>\n",
       "      <td>10.672800</td>\n",
       "      <td>1.193160</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>726.160000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.462760e+05</td>\n",
       "      <td>9.139500e+16</td>\n",
       "      <td>7.304100e+16</td>\n",
       "      <td>17.938700</td>\n",
       "      <td>8.349710e+16</td>\n",
       "      <td>9.208530e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>9.000000</td>\n",
       "      <td>937.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>1.788410e+17</td>\n",
       "      <td>0.903215</td>\n",
       "      <td>23.710200</td>\n",
       "      <td>1.749920</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>776.310000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.818540e+05</td>\n",
       "      <td>1.645770e+17</td>\n",
       "      <td>1.755365e+17</td>\n",
       "      <td>42.080150</td>\n",
       "      <td>1.730690e+17</td>\n",
       "      <td>9.090410e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>9.000000</td>\n",
       "      <td>1249.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>3.161470e+17</td>\n",
       "      <td>1.134360</td>\n",
       "      <td>486.789000</td>\n",
       "      <td>25.041500</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>7344.800000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.569170e+06</td>\n",
       "      <td>3.127350e+17</td>\n",
       "      <td>5.210900e+21</td>\n",
       "      <td>245.140000</td>\n",
       "      <td>2.359760e+20</td>\n",
       "      <td>2.089770e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           atm_model      shower_id         use_id      energyMC  \\\n",
       "count  398227.000000  398227.000000  398227.000000  3.982270e+05   \n",
       "mean        5.250465     624.671403      10.489314  1.238382e+17   \n",
       "std         3.345009     360.883823       5.766413  7.912391e+16   \n",
       "min         1.000000       0.000000       1.000000  3.162770e+16   \n",
       "25%         1.000000     312.000000       5.000000  5.628890e+16   \n",
       "50%         8.000000     625.000000      10.000000  9.965710e+16   \n",
       "75%         9.000000     937.000000      15.000000  1.788410e+17   \n",
       "max         9.000000    1249.000000      20.000000  3.161470e+17   \n",
       "\n",
       "            zenithMC     showerSize  showerSizeError           isT5  \\\n",
       "count  398227.000000  398227.000000    398227.000000  398227.000000   \n",
       "mean        0.677721      16.222010         1.188039       0.671835   \n",
       "std         0.273104      17.278987         0.866914       0.469546   \n",
       "min         0.005836       0.000000         0.000000       0.000000   \n",
       "25%         0.470053       3.482985         0.674117       0.000000   \n",
       "50%         0.698197      10.672800         1.193160       1.000000   \n",
       "75%         0.903215      23.710200         1.749920       1.000000   \n",
       "max         1.134360     486.789000        25.041500       1.000000   \n",
       "\n",
       "               is6T5           Xmax  ...  nearestid    nCandidates  \\\n",
       "count  398227.000000  398227.000000  ...   398227.0  398227.000000   \n",
       "mean        0.671835     731.265860  ...        0.0       7.788909   \n",
       "std         0.469546      97.612257  ...        0.0       5.378215   \n",
       "min         0.000000     541.340000  ...        0.0       0.000000   \n",
       "25%         0.000000     674.485000  ...        0.0       4.000000   \n",
       "50%         1.000000     726.160000  ...        0.0       8.000000   \n",
       "75%         1.000000     776.310000  ...        0.0      12.000000   \n",
       "max         1.000000    7344.800000  ...        0.0      23.000000   \n",
       "\n",
       "                bLDF    isSaturated    muonNumber  electromagneticEnergy  \\\n",
       "count  398227.000000  398227.000000  3.982270e+05           3.982270e+05   \n",
       "mean        0.761453       0.116549  4.892502e+05           1.139049e+17   \n",
       "std         0.426196       0.320883  5.960837e+05           7.390093e+16   \n",
       "min         0.000000       0.000000  3.914030e+03           2.378890e+16   \n",
       "25%         1.000000       0.000000  6.835660e+04           5.133390e+16   \n",
       "50%         1.000000       0.000000  2.462760e+05           9.139500e+16   \n",
       "75%         1.000000       0.000000  6.818540e+05           1.645770e+17   \n",
       "max         1.000000       1.000000  3.569170e+06           3.127350e+17   \n",
       "\n",
       "       photon_energy          s_250  equivalent_energy            M1  \n",
       "count   3.981870e+05  398187.000000       3.979650e+05  3.979650e+05  \n",
       "mean    1.642059e+17      29.006530       1.202942e+17          -inf  \n",
       "std     8.526831e+18      32.499657       6.294315e+17           NaN  \n",
       "min     0.000000e+00       0.000000       0.000000e+00          -inf  \n",
       "25%     0.000000e+00       5.067435       3.012160e+16           NaN  \n",
       "50%     7.304100e+16      17.938700       8.349710e+16  9.208530e-02  \n",
       "75%     1.755365e+17      42.080150       1.730690e+17  9.090410e-01  \n",
       "max     5.210900e+21     245.140000       2.359760e+20  2.089770e+00  \n",
       "\n",
       "[8 rows x 21 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05342499-ec0b-4e89-9a80-b5555c164eb0",
   "metadata": {},
   "source": [
    "### Quality Cuts and Binning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "47c59273-303d-4264-a48f-e101b1c57b96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Events after quality cuts: 209158\n"
     ]
    }
   ],
   "source": [
    "index = index.sample(frac=1)\n",
    "index.loc[index[\"filename\"].str.contains(\"Photon\"), \"isPhoton\"] = 1\n",
    "index.loc[index[\"filename\"].str.contains(\"Proton\"), \"isPhoton\"] = 0\n",
    "\n",
    "index[\"sin2zenith\"] = np.sin(index[\"zenithMC\"]) ** 2\n",
    "\n",
    "# photon efficiency from fit from simulations\n",
    "#index[\"est_efficiency\"] = (\n",
    "#    15.4074\n",
    "#    + 17.4996 * (np.log10(index[\"energyMC\"]) - 17)\n",
    "#    - 12.7485 * index[\"sin2zenith\"]\n",
    "#    - 20.7650 * index[\"sin2zenith\"] ** 2\n",
    "#    - 13.1239 * (np.log10(index[\"energyMC\"]) - 17) * index[\"sin2zenith\"]\n",
    "#)\n",
    "#index[\"est_efficiency\"] = expit(index[\"est_efficiency\"])\n",
    "\n",
    "feature_filters = {\n",
    "    \"zenithMC\": {\"filter_type\": \"range\", \"max_cut\": np.deg2rad(45)},\n",
    "    \"photon_energy\": {\"filter_type\": \"range\", \"min_cut\": 1},\n",
    "    #\"est_efficiency\": {\"filter_type\": \"range\", \"min_cut\": 0.9},\n",
    "    \"isT5\": {\"filter_type\": \"value\", \"value_to_keep\": 1}\n",
    "}\n",
    "index = filter_dataframe(index, feature_filters)\n",
    "\n",
    "index, e_bin_centers, e_bin_edges, e_labels = create_bins(\n",
    "    index,\n",
    "    lower_val=10**16.5,\n",
    "    upper_val=10**17.5,\n",
    "    num=6,\n",
    "    unbinned_col=\"energyMC\",\n",
    "    bin_column_name=\"e_bin\",\n",
    "    bin_width=\"equal_logarithmic\",\n",
    ")\n",
    "\n",
    "index, z_bin_centers, z_bin_edges, z_labels = create_bins(\n",
    "    index,\n",
    "    lower_val=0,\n",
    "    upper_val=np.sin(np.deg2rad(45)) ** 2,\n",
    "    num=4,\n",
    "    unbinned_col=\"sin2zenith\",\n",
    "    bin_column_name=\"z_bin\",\n",
    "    bin_width=\"equal\",\n",
    ")\n",
    "\n",
    "index = index.loc[~index[\"e_bin\"].isnull()]\n",
    "\n",
    "# corrupted or problematic ADSTs\n",
    "exclude_list = [\n",
    "\"Photon_17.0_17.5_011102_11\",\n",
    "\"Photon_17.0_17.5_080595_20\"\n",
    "]\n",
    "index = index[~index['filename'].isin(exclude_list)]\n",
    "\n",
    "print(f\"Events after quality cuts: {len(index)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "254cccb4-87f7-4281-9024-05fb739a2232",
   "metadata": {},
   "source": [
    "### Balanced Dataset Division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ece99096-6077-4fee-ba31-449995b8df5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 117651\n",
      "Validation dataset size: 39217\n",
      "Test dataset size: 52290\n"
     ]
    }
   ],
   "source": [
    "# Combine label and the two categorical variables for stratified sampling\n",
    "index[\"categorical_balance\"] = (\n",
    "    index[\"isPhoton\"].astype(str)\n",
    "    + \"_\"\n",
    "    + index[\"e_bin\"].astype(str)\n",
    "    + \"_\"\n",
    "    + index[\"z_bin\"].astype(str)\n",
    ")\n",
    "\n",
    "random_seed = 42\n",
    "stratified_split = StratifiedShuffleSplit(\n",
    "    n_splits=1, test_size=0.25, random_state=random_seed\n",
    ")\n",
    "\n",
    "for dev_index_, test_index_ in stratified_split.split(\n",
    "    index, index[\"categorical_balance\"]\n",
    "):\n",
    "    # Original Training set\n",
    "    dev_index = index.iloc[dev_index_]\n",
    "\n",
    "    # Testing set\n",
    "    test_index = index.iloc[test_index_]\n",
    "\n",
    "# Further split the original training set into train and validation sets\n",
    "validation_size = 0.25  # Adjust as needed\n",
    "split = StratifiedShuffleSplit(\n",
    "    n_splits=1, test_size=validation_size, random_state=random_seed\n",
    ")\n",
    "\n",
    "for train_index_, validation_index_ in split.split(\n",
    "    dev_index, dev_index[\"categorical_balance\"]\n",
    "):\n",
    "    train_index = dev_index.iloc[train_index_]\n",
    "    validation_index = dev_index.iloc[validation_index_]\n",
    "\n",
    "# Print the size of each dataset\n",
    "print(\"Train dataset size:\", train_index.shape[0])\n",
    "print(\"Validation dataset size:\", validation_index.shape[0])\n",
    "print(\"Test dataset size:\", test_index.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "63fb3213-ccb9-4512-8bb9-de2d6ec6685f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_path = \"/sps/pauger/users/erodriguez/PhotonDiscrimination/JSONfiles/\"\n",
    "root_path = \"/sps/pauger/users/erodriguez/PhotonDiscrimination/root/\"\n",
    "\n",
    "# Function to construct paths based on DataFrame columns\n",
    "def construct_path(row, base_path):\n",
    "    return f\"{base_path}{row['mass_group']}/{row['filename']}.json\"\n",
    "\n",
    "# Set paths according to index\n",
    "train_paths = train_index.apply(lambda row: construct_path(row, dir_path), axis=1).tolist()\n",
    "val_paths = validation_index.apply(lambda row: construct_path(row, dir_path), axis=1).tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba47953c-55f1-4086-a7a1-9a80b4af0194",
   "metadata": {},
   "source": [
    "### Generation of Normalization Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9fb598c7-7116-4e02-8222-b09ae9649e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_dict_computed = True\n",
    "augmentation_and_normalization_options={\"mask_PMTs\":True,\n",
    "                                        \"AoP_and_saturation\":True,\n",
    "                                        \"log_normalize_traces\":True,\n",
    "                                        \"log_normalize_signals\":True,\n",
    "                                        \"mask_MD_mods\":True}\n",
    "\n",
    "if not norm_dict_computed:\n",
    "    # process the dataset\n",
    "    train_PyG_ds = SD433UMDatasetHomogeneous(\n",
    "        file_paths=train_paths,\n",
    "        root=root_path,\n",
    "        augmentation_options=augmentation_and_normalization_options)\n",
    "    # compute statistics required for standardization\n",
    "    normalization_dict = train_PyG_ds.compute_normalization_params(features=[\"x\",\n",
    "                                                                             \"y\",\n",
    "                                                                             \"z\",\n",
    "                                                                             \"deltaTimeHottest\",\n",
    "                                                                             \"WCD_signal\"])\n",
    "    # set values for min-max normalization\n",
    "    normalization_dict[\"pmt_number\"] = {\"min\": 1,\n",
    "                                        \"max\": 3,\n",
    "                                        \"method\": \"min_max_scaling\"}\n",
    "    normalization_dict[\"effective_area\"] = {\"min\": 0,\n",
    "                                            \"max\": 3 * 10.46,\n",
    "                                            \"method\": \"min_max_scaling\"}\n",
    "    normalization_dict[\"rho_mu\"] = {'min': -2.0,\n",
    "                                    'method': 'min_max_scaling',\n",
    "                                    'max': (64*3)/(3*10*np.cos(np.deg2rad(45)))}\n",
    "    # print the dict to overwrite the code below\n",
    "    pprint(normalization_dict)\n",
    "else:\n",
    "    normalization_dict = {\n",
    "                         # with silent\n",
    "                         #'deltaTimeHottest': {'mean': -13.743452072143555,\n",
    "                         #                     'method': 'standardization',\n",
    "                         #                     'std': 446.0340270996094},\n",
    "                         # without silent\n",
    "                         'deltaTimeHottest': {'mean': -47.27531568592806,\n",
    "                                              'method': 'standardization',\n",
    "                                              'std': 603.6062483849028},\n",
    "                         'effective_area': {'max': 31.380000000000003,\n",
    "                                            'method': 'min_max_scaling',\n",
    "                                            'min': 0},\n",
    "                         'pmt_number': {'max': 3, 'method': 'min_max_scaling', 'min': 1},\n",
    "                         #'rho_mu': {'min': -2.0,\n",
    "                         #           'method': 'min_max_scaling',\n",
    "                         #           'max': (64*3)/(3*10*np.cos(np.deg2rad(45)))},\n",
    "                         'rho_mu':{'mean': 0.21510971141596952,\n",
    "                                   'method': 'standardization',\n",
    "                                   'std': 0.9057438827119321},\n",
    "                         # with silent\n",
    "                         #'x': {'mean': -2.081512212753296,\n",
    "                         #      'method': 'standardization',\n",
    "                         #      'std': 779.2147827148438},\n",
    "                         #'y': {'mean': 0.3692401945590973,\n",
    "                         #      'method': 'standardization',\n",
    "                         #      'std': 779.95458984375},\n",
    "                         #'z': {'mean': -0.17404018342494965,\n",
    "                         #      'method': 'standardization',\n",
    "                         #      'std': 9.372444152832031},\n",
    "                         # without silent\n",
    "                         'x': {'mean': -1.01887806305201,\n",
    "                               'method': 'standardization',\n",
    "                               'std': 364.5171135403138},\n",
    "                         'y': {'mean': -0.06542848666299515,\n",
    "                               'method': 'standardization',\n",
    "                               'std': 356.8056580363697},\n",
    "                         'z': {'mean': -0.16543275617686548,\n",
    "                               'method': 'standardization',\n",
    "                               'std': 6.410170534866934}\n",
    "                        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc7f41f-016b-48e4-8a89-287cef065802",
   "metadata": {},
   "source": [
    "### Datasets and Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a0fe606f-1196-438a-a504-f0943348f94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_run = False\n",
    "include_silent = False\n",
    "dual_input = True\n",
    "loss_strategy = \"equal\"  # Options: \"equal\", \"prioritize_sdmd\", \"alternate\"\n",
    "# Model ID and file paths\n",
    "model_id = f\"dual_input_{str(dual_input)}_silent_{str(include_silent)}_loss_strategy_{str(loss_strategy)}_post_UHECR\"\n",
    "model_filename = f'{model_id}.pth'\n",
    "last_model_filename = f'last_{model_id}.pth'\n",
    "csv_filename = f\"learning_curves_{model_id}.csv\"\n",
    "\n",
    "if test_run:\n",
    "    # subset for code testing\n",
    "    train_paths = train_paths[:55000]\n",
    "    val_paths = val_paths#[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e395247-81d9-4674-bf69-9994b13d972b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Divide the dataset paths into subsets\n",
    "n_train_loaders = 5\n",
    "n_val_loaders = 3\n",
    "train_subset_size = len(train_paths) // n_train_loaders\n",
    "train_subset_paths = [train_paths[i * train_subset_size: (i + 1) * train_subset_size] for i in range(n_train_loaders)]\n",
    "val_subset_size = len(val_paths) // n_val_loaders\n",
    "val_subset_paths = [val_paths[i * val_subset_size: (i + 1) * val_subset_size] for i in range(n_val_loaders)]\n",
    "\n",
    "# Create datasets and loaders for each subset\n",
    "# This is needed to speed up training\n",
    "# One huge Dataset for trainig, even with the loader, seems to slow down the training process\n",
    "train_loaders = []\n",
    "val_loaders = []\n",
    "n_time_bins = 60\n",
    "\n",
    "for subset_paths in train_subset_paths:\n",
    "    train_loader = DataLoader(SD433UMDatasetHomogeneous(file_paths=subset_paths,\n",
    "                              root=root_path,\n",
    "                              augmentation_options=augmentation_and_normalization_options,\n",
    "                              normalization_dict=normalization_dict,\n",
    "                              include_silent=include_silent, n_time_bins=n_time_bins),\n",
    "                              batch_size=32, shuffle=True, num_workers=8)\n",
    "    train_loaders.append(train_loader)\n",
    "\n",
    "for subset_paths in val_subset_paths:\n",
    "    val_loader = DataLoader(SD433UMDatasetHomogeneous(file_paths=subset_paths,\n",
    "                              root=root_path,\n",
    "                              augmentation_options=augmentation_and_normalization_options,\n",
    "                              normalization_dict=normalization_dict,\n",
    "                              include_silent=include_silent, n_time_bins=n_time_bins),\n",
    "                              batch_size=32, num_workers=8,)\n",
    "    val_loaders.append(val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ded320-43a7-481d-9db4-89465d78e18d",
   "metadata": {},
   "source": [
    "## GAN compilation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aedb2e67-f366-4228-9029-48bd44224b34",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'n_time_bins' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# TA definition\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m TA_input_length \u001b[38;5;241m=\u001b[39m \u001b[43mn_time_bins\u001b[49m  \u001b[38;5;66;03m# Length of the input trace\u001b[39;00m\n\u001b[1;32m      3\u001b[0m TA_initial_kernel_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m7\u001b[39m  \u001b[38;5;66;03m# Initial kernel size for the first convolutional layer\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# 120 bins\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m#TA_filters = [64, 32, 4]\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m#TA_kernel_sizes = [7, 7, 8]\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m#TA_strides = [3, 4, 1]\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# 60 bins\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'n_time_bins' is not defined"
     ]
    }
   ],
   "source": [
    "# TA definition\n",
    "TA_input_length = n_time_bins  # Length of the input trace\n",
    "TA_initial_kernel_size = 7  # Initial kernel size for the first convolutional layer\n",
    "\n",
    "# 120 bins\n",
    "#TA_filters = [64, 32, 4]\n",
    "#TA_kernel_sizes = [7, 7, 8]\n",
    "#TA_strides = [3, 4, 1]\n",
    "\n",
    "# 60 bins\n",
    "TA_filters = [64, 32, 4]\n",
    "TA_kernel_sizes = [7, 6, 5]\n",
    "TA_strides = [3, 3, 1]\n",
    "\n",
    "# arguments\n",
    "sd_node_features = 5\n",
    "sdmd_node_features = 7\n",
    "md_node_features = 6\n",
    "GCN_filters = [16, 32, 4]\n",
    "dense_units = [16, 8]\n",
    "num_heads = 3  # Number of attention heads to be used in GATv2Conv layers\n",
    "\n",
    "nn_args = {\n",
    "        \"TA_filters\":TA_filters,\n",
    "        \"TA_kernel_sizes\":TA_kernel_sizes,\n",
    "        \"TA_strides\":TA_strides,\n",
    "        \"dense_units\":dense_units,\n",
    "        \"GCN_filters\":GCN_filters,\n",
    "        \"sd_node_features\":sd_node_features,\n",
    "        \"md_node_features\":md_node_features,\n",
    "        \"sdmd_node_features\":sdmd_node_features,\n",
    "        \"num_heads\":num_heads}\n",
    "\n",
    "if dual_input:\n",
    "    # Instantiate the GNNWithAttentionDiscriminator\n",
    "    triheaded_model = GNNWithAttentionDiscriminator3HeadsDualInput(**nn_args)\n",
    "else:\n",
    "    # Instantiate the GNNWithAttentionDiscriminator\n",
    "    triheaded_model = GNNWithAttentionDiscriminator3Heads(**nn_args)\n",
    "\n",
    "print(\"Parameters: \", sum(p.numel() for p in triheaded_model.parameters()))\n",
    "triheaded_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "022a405a-32a4-4d06-9ab5-dec48b19070a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training from scratch.\n"
     ]
    }
   ],
   "source": [
    "# Check if the model exists to load it, otherwise initialize a new model\n",
    "if os.path.exists(model_filename):\n",
    "    # Load existing model and optimizer state\n",
    "    triheaded_model = torch.load(model_filename)\n",
    "    optimizer = torch.optim.Adam(triheaded_model.parameters(), lr=1e-4)  # Recreate optimizer\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3, verbose=True)\n",
    "    \n",
    "    # Optional: load optimizer state and scheduler state if you have saved them\n",
    "    checkpoint = torch.load(f'{model_id}_checkpoint.pth')\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "    start_epoch = checkpoint['epoch'] + 1\n",
    "    best_val_ROC_AUC = checkpoint['best_val_ROC_AUC']\n",
    "    print(f\"Resuming training from epoch {start_epoch} with best validation ROC-AUC {best_val_ROC_AUC:.4f}.\")\n",
    "else:\n",
    "    # Initialize model, optimizer, and scheduler from scratch\n",
    "    triheaded_model.train()\n",
    "    optimizer = torch.optim.Adam(triheaded_model.parameters(), lr=1e-4)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, verbose=True)\n",
    "    start_epoch = 0\n",
    "    best_val_ROC_AUC = 0.0\n",
    "    print(\"Starting training from scratch.\")\n",
    "    \n",
    "    # Initialize CSV file and write header if starting fresh\n",
    "    with open(csv_filename, mode='w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(['epoch', 'time',\n",
    "                         'train_roc_auc_sd', 'train_roc_auc_sdmd', 'train_roc_auc_md',\n",
    "                         'train_loss_sd', 'train_loss_sdmd', 'train_loss_md',\n",
    "                         'val_roc_auc_sd', 'val_roc_auc_sdmd', 'val_roc_auc_md',\n",
    "                         'val_loss_sd', 'val_loss_sdmd', 'val_loss_md'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "df1710b8-c71c-4b68-804d-4d6d3a84c87c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early stopping parameters\n",
    "patience = 120\n",
    "counter = 0\n",
    "num_epochs = 120\n",
    "\n",
    "# transformations for individual input\n",
    "masking_transformation_sd =  MaskNodes(max_nodes2prune=2)\n",
    "masking_transformation_md =  MaskMdCounters(rho_mu_column_idx=7, effective_area_column_idx=6, silent_value=1/((64*3)/(3*10*np.cos(np.deg2rad(45)))-(-2.0)))\n",
    "\n",
    "# transformations for dual input\n",
    "silent_prunner = SilentPrunner(silent_col_index = 4, silent_value=0.0)\n",
    "random_masking = MaskRandomNodes(max_nodes2prune=2)\n",
    "\n",
    "# loss\n",
    "criterion = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "47bf0a4c-0eb6-4554-b66b-12f3f37f92db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weighting configuration based on the chosen strategy\n",
    "def get_loss_weights(epoch, strategy):\n",
    "    if strategy == \"equal\":\n",
    "        # Equal weighting for all heads\n",
    "        return {'sd': 1.0, 'sdmd': 1.0, 'md': 1.00}\n",
    "    \n",
    "    elif strategy == \"prioritize_sdmd\":\n",
    "        # More importance to SDMD (middle head)\n",
    "        return {'sd': 0.2, 'sdmd': 0.6, 'md': 0.2}\n",
    "    \n",
    "    elif strategy == \"alternate\":\n",
    "        # Alternating focus with SDMD every other epoch\n",
    "        if epoch % 4 == 0:  # Focus on SD head (epoch 0, 4, 8, ...)\n",
    "            return {'sd': 1.0, 'sdmd': 0.0, 'md': 0.0}\n",
    "        elif epoch % 4 == 1:  # Focus on SDMD head (epoch 1, 5, 9, ...)\n",
    "            return {'sd': 0.0, 'sdmd': 1.0, 'md': 0.0}\n",
    "        elif epoch % 4 == 2:  # Focus on MD head (epoch 2, 6, 10, ...)\n",
    "            return {'sd': 0.0, 'sdmd': 0.0, 'md': 1.0}\n",
    "        else:  # Focus again on SDMD head (epoch 3, 7, 11, ...)\n",
    "            return {'sd': 0.0, 'sdmd': 1.0, 'md': 0.0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b14a8e7-1f88-4cbe-84da-4564fab0371d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Subset 1 Training: 100%|██████████| 736/736 [08:13<00:00,  1.49it/s]\n",
      "Subset 2 Training: 100%|██████████| 736/736 [08:19<00:00,  1.47it/s]\n",
      "Subset 3 Training: 100%|██████████| 736/736 [08:23<00:00,  1.46it/s]\n",
      "Subset 4 Training:  49%|████▊     | 358/736 [04:02<02:55,  2.16it/s]"
     ]
    }
   ],
   "source": [
    "###################\n",
    "## Training loop ##\n",
    "###################\n",
    "\n",
    "# would be so nice to just have a .fit() >:(\n",
    "\n",
    "for epoch in tqdm(range(start_epoch, num_epochs), desc=\"Epoch\", position=0, leave=True):\n",
    "    \n",
    "    start_time = time.time()  # Record start time of epoch\n",
    "    # Get the current loss weights based on the chosen strategy\n",
    "    loss_weights = get_loss_weights(epoch, loss_strategy)\n",
    "    \n",
    "    # Initialize containers for training predictions and targets\n",
    "    train_preds_sd = []\n",
    "    train_preds_sdmd = []\n",
    "    train_preds_md = []\n",
    "    train_targets = []\n",
    "    \n",
    "    # Training over subsets\n",
    "    for i, train_loader in enumerate(train_loaders):\n",
    "        subset_preds_sd = []\n",
    "        subset_preds_sdmd = []\n",
    "        subset_preds_md = []\n",
    "        subset_targets = []\n",
    "        \n",
    "        for batch_idx, data in enumerate(tqdm(train_loader, desc=f\"Subset {i+1} Training\", position=0, leave=True)):\n",
    "            try:\n",
    "                # Apply transformations\n",
    "                data_sd = masking_transformation_sd.collate(data)\n",
    "                if dual_input:\n",
    "                    # dual input\n",
    "                    #data_sd_no_silent = silent_prunner.collate(data_sd)\n",
    "                    #data_sdmd = random_masking.collate(data_sd_no_silent)\n",
    "                    data_sdmd = random_masking.collate(data_sd)\n",
    "                    gpu_sd_graph = data_sd.to(device)\n",
    "                    gpu_sdmd_graph = data_sdmd.to(device)\n",
    "                    inputs, targets = [gpu_sd_graph, gpu_sdmd_graph], gpu_sd_graph.y\n",
    "                else:\n",
    "                    # single input\n",
    "                    data_sdmd = masking_transformation_md.collate(data_sd)\n",
    "                    gpu_graph = data_sdmd.to(device)\n",
    "                    inputs, targets = gpu_graph, gpu_graph.y\n",
    "    \n",
    "                # Sanity checks on inputs and targets\n",
    "                assert torch.all(torch.isfinite(data_sd.x)), \"NaN detected in data_sd.x\"\n",
    "                assert torch.all(torch.isfinite(data_sdmd.x)), \"NaN detected in data_sdmd.x\"\n",
    "                assert torch.all(torch.isfinite(data_sd.x_traces)), \"NaN detected in data_sd.x_traces\"\n",
    "                assert torch.all(torch.isfinite(data_sdmd.x_traces)), \"NaN detected in data_sd.x_traces\"\n",
    "                assert torch.all(torch.isfinite(targets)), \"NaN detected in targets\"\n",
    "\n",
    "            except AssertionError as e:\n",
    "                # Print the error message and skip this batch\n",
    "                print(f\"Skipping batch {batch_idx} in subset {i+1} due to assertion error: {e}\")\n",
    "                continue\n",
    "            \n",
    "            # Training step\n",
    "            optimizer.zero_grad()\n",
    "            output1, output2, output3 = triheaded_model(inputs)\n",
    "\n",
    "            # Compute the weighted losses for each head based on the current strategy\n",
    "            loss_sd = criterion(output1.flatten(), targets.flatten().float()) * loss_weights['sd']\n",
    "            loss_sdmd = criterion(output2.flatten(), targets.flatten().float()) * loss_weights['sdmd']\n",
    "            loss_md = criterion(output3.flatten(), targets.flatten().float()) * loss_weights['md']\n",
    "\n",
    "            # Additional NaN checks\n",
    "            if torch.isnan(loss_sd) or torch.isnan(loss_sdmd) or torch.isnan(loss_md):\n",
    "                print(f\"NaN detected in loss components: SD: {loss_sd}, SDMD: {loss_sdmd}, MD: {loss_md}\")\n",
    "                continue  # Skip this batch\n",
    "\n",
    "            # Total loss\n",
    "            total_loss = loss_sd + loss_sdmd + loss_md\n",
    "            with torch.autograd.set_detect_anomaly(True):\n",
    "                total_loss.backward()\n",
    "            clip_grad_norm_(triheaded_model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Save predictions and targets\n",
    "            subset_preds_sd.append(output1.detach().cpu())\n",
    "            subset_preds_sdmd.append(output2.detach().cpu())\n",
    "            subset_preds_md.append(output3.detach().cpu())\n",
    "            subset_targets.append(targets.detach().cpu())\n",
    "        \n",
    "        # appending to higher level container\n",
    "        train_preds_sd.append(torch.cat(subset_preds_sd))\n",
    "        train_preds_sdmd.append(torch.cat(subset_preds_sdmd))\n",
    "        train_preds_md.append(torch.cat(subset_preds_md))\n",
    "        train_targets.append(torch.cat(subset_targets))\n",
    "    \n",
    "    # Convert predictions and targets to tensors\n",
    "    train_preds_sd = torch.cat(train_preds_sd)\n",
    "    train_preds_sdmd = torch.cat(train_preds_sdmd)\n",
    "    train_preds_md = torch.cat(train_preds_md)\n",
    "    train_targets = torch.cat(train_targets)\n",
    "    \n",
    "    # Calculate loss for train set\n",
    "    train_loss_sd = criterion(train_preds_sd, train_targets.unsqueeze(1).float())\n",
    "    train_loss_sdmd = criterion(train_preds_sdmd, train_targets.unsqueeze(1).float())\n",
    "    train_loss_md = criterion(train_preds_md, train_targets.unsqueeze(1).float())\n",
    "    train_ROC_AUC_sd = roc_auc_score(train_targets.numpy(), train_preds_sd.numpy())\n",
    "    train_ROC_AUC_sdmd = roc_auc_score(train_targets.numpy(), train_preds_sdmd.numpy())\n",
    "    train_ROC_AUC_md = roc_auc_score(train_targets.numpy(), train_preds_md.numpy())\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}: Train ROC-AUC SD: {train_ROC_AUC_sd.item():.4f}, Train Loss SD: {train_loss_sd.item():.4f}\")\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}: Train ROC-AUC SDMD: {train_ROC_AUC_sdmd.item():.4f}, Train Loss SDMD: {train_loss_sdmd.item():.4f}\")\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}: Train ROC-AUC MD: {train_ROC_AUC_md.item():.4f}, Train Loss MD: {train_loss_md.item():.4f}\")\n",
    "    \n",
    "    del train_preds_sd, train_preds_sdmd, train_targets\n",
    "    \n",
    "    # Validation loop\n",
    "    val_preds_sd = []\n",
    "    val_preds_sdmd = []\n",
    "    val_preds_md = []\n",
    "    val_targets = []\n",
    "    \n",
    "    for i, val_loader in enumerate(val_loaders):\n",
    "        subset_preds_sd = []\n",
    "        subset_preds_sdmd = []\n",
    "        subset_preds_md = []\n",
    "        subset_targets = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_idx, data in enumerate(val_loader):\n",
    "                try:\n",
    "                    data_sd = masking_transformation_sd.collate(data)\n",
    "                    if dual_input:\n",
    "                        # dual input\n",
    "                        #data_sd_no_silent = silent_prunner.collate(data_sd)\n",
    "                        #data_sdmd = random_masking.collate(data_sd_no_silent)\n",
    "                        data_sdmd = random_masking.collate(data_sd)\n",
    "                        gpu_sd_graph = data_sd.to(device)\n",
    "                        gpu_sdmd_graph = data_sdmd.to(device)\n",
    "                        inputs, targets = [gpu_sd_graph, gpu_sdmd_graph], gpu_sd_graph.y\n",
    "                    else:\n",
    "                        # single input\n",
    "                        data_sdmd = masking_transformation_md.collate(data_sd)\n",
    "                        gpu_graph = data_sdmd.to(device)\n",
    "                        inputs, targets = gpu_graph, gpu_graph.y\n",
    "\n",
    "                    # Sanity checks on inputs and targets\n",
    "                    assert torch.all(torch.isfinite(data_sd.x)), \"NaN detected in data_sd.x\"\n",
    "                    assert torch.all(torch.isfinite(data_sdmd.x)), \"NaN detected in data_sdmd.x\"\n",
    "                    assert torch.all(torch.isfinite(data_sd.x_traces)), \"NaN detected in data_sd.x_traces\"\n",
    "                    assert torch.all(torch.isfinite(data_sdmd.x_traces)), \"NaN detected in data_sd.x_traces\"\n",
    "                    assert torch.all(torch.isfinite(targets)), \"NaN detected in targets\"\n",
    "    \n",
    "                    output1, output2, output3 = triheaded_model(inputs)\n",
    "                    subset_preds_sd.append(output1.detach().cpu())\n",
    "                    subset_preds_sdmd.append(output2.detach().cpu())\n",
    "                    subset_preds_md.append(output3.detach().cpu())\n",
    "                    subset_targets.append(targets.detach().cpu())\n",
    "                except AssertionError as e:\n",
    "                    # Print the error message and skip this batch\n",
    "                    print(f\"Skipping batch {batch_idx} in subset {i+1} due to assertion error: {e}\")\n",
    "                    continue\n",
    "            \n",
    "        val_preds_sd.append(torch.cat(subset_preds_sd))\n",
    "        val_preds_sdmd.append(torch.cat(subset_preds_sdmd))\n",
    "        val_preds_md.append(torch.cat(subset_preds_md))\n",
    "        val_targets.append(torch.cat(subset_targets))\n",
    "\n",
    "    \n",
    "    # Concatenate validation predictions and targets\n",
    "    val_preds_sd = torch.cat(val_preds_sd)\n",
    "    val_preds_sdmd = torch.cat(val_preds_sdmd)\n",
    "    val_preds_md = torch.cat(val_preds_md)\n",
    "    val_targets = torch.cat(val_targets)\n",
    "    \n",
    "    # Calculate validation metrics\n",
    "    val_loss_sd = criterion(val_preds_sd, val_targets.unsqueeze(1).float())\n",
    "    val_loss_sdmd = criterion(val_preds_sdmd, val_targets.unsqueeze(1).float())\n",
    "    val_loss_md = criterion(val_preds_md, val_targets.unsqueeze(1).float())\n",
    "    val_ROC_AUC_sd = roc_auc_score(val_targets.numpy(), val_preds_sd.numpy())\n",
    "    val_ROC_AUC_sdmd = roc_auc_score(val_targets.numpy(), val_preds_sdmd.numpy())\n",
    "    val_ROC_AUC_md = roc_auc_score(val_targets.numpy(), val_preds_md.numpy())\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}: Val ROC-AUC SD: {val_ROC_AUC_sd.item():.4f}, Val Loss SD: {val_loss_sd.item():.4f}\")\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}: Val ROC-AUC SDMD: {val_ROC_AUC_sdmd.item():.4f}, Val Loss SDMD: {val_loss_sdmd.item():.4f}\")\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}: Val ROC-AUC MD: {val_ROC_AUC_md.item():.4f}, Val Loss MD: {val_loss_md.item():.4f}\")\n",
    "    \n",
    "    del val_preds_sd, val_preds_sdmd, val_targets\n",
    "    \n",
    "    # Append training and validation metrics to the CSV file\n",
    "    with open(csv_filename, mode='a', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([epoch + 1, time.time() - start_time,\n",
    "                         train_ROC_AUC_sd, train_ROC_AUC_sdmd, train_ROC_AUC_md,\n",
    "                         train_loss_sd.item(), train_loss_sdmd.item(), train_loss_md.item(),\n",
    "                         val_ROC_AUC_sd, val_ROC_AUC_sdmd, val_ROC_AUC_md,\n",
    "                         \n",
    "                         val_loss_sd.item(), val_loss_sdmd.item(), val_loss_md.item()])\n",
    "    \n",
    "    # Step the scheduler\n",
    "    scheduler.step(val_loss_sdmd.item())\n",
    "    \n",
    "    # Early stopping and saving the best model\n",
    "    if val_ROC_AUC_sdmd > best_val_ROC_AUC:\n",
    "        best_val_ROC_AUC = val_ROC_AUC_sdmd\n",
    "        counter = 0\n",
    "        torch.save(triheaded_model, model_filename)\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': triheaded_model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            'best_val_ROC_AUC': best_val_ROC_AUC\n",
    "        }, f'{model_id}_checkpoint.pth')\n",
    "    else:\n",
    "        counter += 1\n",
    "        if counter >= patience:\n",
    "            print(\"Early stopping: No improvement in validation accuracy.\")\n",
    "            break\n",
    "    \n",
    "    gc.collect()\n",
    "    torch.save(triheaded_model, last_model_filename)\n",
    "\n",
    "print(\"Training complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab603dc-09f8-4b82-a022-fa34acdf4e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyG",
   "language": "python",
   "name": "pyg"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
